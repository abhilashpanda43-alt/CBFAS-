{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Confidence-Based Filtering with Adaptive Sampling (CBFAS) Method"
      ],
      "metadata": {
        "id": "voIi9LABKUSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Basic Libraries"
      ],
      "metadata": {
        "id": "iTqxTekoL3CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as ml\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "D9t3YDtBCKgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6b6c422"
      },
      "source": [
        "### Importing Machine Learning Modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "L5m0XmkICREz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba9f2149"
      },
      "source": [
        "### Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset0=pd.read_excel(\"/content/RCOMSLW+.xlsx\")"
      ],
      "metadata": {
        "id": "9GhAJYcPCRrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75b6eef3"
      },
      "source": [
        "### Dividing Dataset into Independent and Dependent Features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import ValuesView\n",
        "X = dataset0.iloc[:,1:19]\n",
        "y = dataset0.iloc[:,21]"
      ],
      "metadata": {
        "id": "YuwgKYHeCS25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35084112"
      },
      "source": [
        "### Encoding Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "hduO34s2CWBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dab03542"
      },
      "source": [
        "### Dropping Unnecessary Features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=X.drop(['FLOW/MAXFLOW','LTYPE','GTYPE','LO','DIAGONAL','WIDTH','LA','PHONE','MIDDLE'],axis=1)"
      ],
      "metadata": {
        "id": "dDi_fzVhCWps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf3f837c"
      },
      "source": [
        "### Defining Minority Point Zone Classification Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def classify_minority_point_zone(minority_point, X, y, k):\n",
        "    \"\"\"\n",
        "    Classifies a minority point into 'safe', 'dangerous', or 'noise' zone\n",
        "    based on its k-nearest neighbors.\n",
        "\n",
        "    Args:\n",
        "        minority_point (np.ndarray): A single data point from the minority class.\n",
        "        X (np.ndarray): The complete dataset.\n",
        "        y (np.ndarray): The corresponding labels.\n",
        "        k (int): The number of nearest neighbors to consider.\n",
        "\n",
        "    Returns:\n",
        "        str: The zone of the minority point (\"safe\", \"dangerous\", or \"noise\").\n",
        "    \"\"\"\n",
        "    distances = np.array([euclidean(minority_point, x) for x in X])\n",
        "    # Get indices of sorted distances, excluding the point itself\n",
        "    nearest_indices = np.argsort(distances)[1:k+1]\n",
        "    nearest_labels = y[nearest_indices]\n",
        "    minority_class = 1 # Assuming minority class is labeled as 1\n",
        "    majority_class = 0 # Assuming majority class is labeled as 0\n",
        "\n",
        "    minority_neighbors_count = np.sum(nearest_labels == minority_class)\n",
        "    majority_neighbors_count = np.sum(nearest_labels == majority_class)\n",
        "\n",
        "    # Refined classification logic based on user's description:\n",
        "    # Noise: Points primarily surrounded by majority neighbors (near majority core)\n",
        "    if majority_neighbors_count >= k * 0.8: # Consider a point noise if at least 80% of neighbors are majority\n",
        "         return \"noise\"\n",
        "    # Safe: Points with a high proportion of minority neighbors (away from majority)\n",
        "    elif minority_neighbors_count >= k * 0.8: # Consider a point safe if at least 80% of neighbors are minority\n",
        "         return \"safe\"\n",
        "    # Dangerous: Points near the boundary, with a mix of majority and minority neighbors\n",
        "    else:\n",
        "        return \"dangerous\""
      ],
      "metadata": {
        "id": "XhPd_v7qCXdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90f942be"
      },
      "source": [
        "### Defining Majority Point Zone Classification Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_majority_point_zone(majority_point, X, y, k):\n",
        "    \"\"\"\n",
        "    Classifies a majority point into 'safe', 'dangerous', or 'noise' zone\n",
        "    based on its k-nearest neighbors.\n",
        "\n",
        "    Args:\n",
        "        majority_point (np.ndarray): A single data point from the majority class.\n",
        "        X (np.ndarray): The complete dataset.\n",
        "        y (np.ndarray): The corresponding labels.\n",
        "        k (int): The number of nearest neighbors to consider.\n",
        "\n",
        "    Returns:\n",
        "        str: The zone of the majority point (\"safe\", \"dangerous\", or \"noise\").\n",
        "    \"\"\"\n",
        "    distances = np.array([euclidean(majority_point, x) for x in X])\n",
        "    # Get indices of sorted distances, excluding the point itself\n",
        "    nearest_indices = np.argsort(distances)[1:k+1]\n",
        "    nearest_labels = y[nearest_indices]\n",
        "    minority_class = 1 # Assuming minority class is labeled as 1\n",
        "    majority_class = 0 # Assuming majority class is labeled as 0\n",
        "\n",
        "    minority_neighbors_count = np.sum(nearest_labels == minority_class)\n",
        "    majority_neighbors_count = np.sum(nearest_labels == majority_class)\n",
        "\n",
        "    # Classification logic for majority points:\n",
        "    # Noise: Points primarily surrounded by minority neighbors (near minority core)\n",
        "    if minority_neighbors_count >= k * 0.8: # Consider a point noise if at least 80% of neighbors are minority\n",
        "         return \"noise\"\n",
        "    # Safe: Points with a high proportion of majority neighbors (away from minority)\n",
        "    elif majority_neighbors_count >= k * 0.8: # Consider a point safe if at least 80% of neighbors are majority\n",
        "         return \"safe\"\n",
        "    # Dangerous: Points near the boundary, with a mix of majority and minority neighbors\n",
        "    else:\n",
        "        return \"dangerous\""
      ],
      "metadata": {
        "id": "lckONHPwCY0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81b50cb7"
      },
      "source": [
        "### Calculating and Plotting Minority Zone Classifications"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate zone classifications for k = 4 with the updated data and logic\n",
        "minority_indices = np.where(y == 1)[0]\n",
        "zone_classifications = {}\n",
        "k = 4\n",
        "zones_for_k = []\n",
        "\n",
        "# Convert X to a NumPy array for consistent indexing\n",
        "X_np = X.values\n",
        "\n",
        "for i in minority_indices:\n",
        "    # Access the row by integer position using NumPy indexing from the NumPy array X_np\n",
        "    minority_point = X_np[i]\n",
        "    # Pass the NumPy array X_np to the classification function\n",
        "    zone = classify_minority_point_zone(minority_point, X_np, y, k)\n",
        "    zones_for_k.append(zone)\n",
        "zone_classifications[k] = zones_for_k\n",
        "\n",
        "# Plot the results for k = 4\n",
        "for k, zones in zone_classifications.items():\n",
        "    plt.figure(figsize=(10, 6)) # Increased figure size to accommodate legend outside\n",
        "    # Plot majority class (using the first two columns of the modified X_np)\n",
        "    # Ensure there are at least two columns before plotting\n",
        "    if X_np.shape[1] >= 2:\n",
        "        plt.scatter(X_np[y == 0, 0], X_np[y == 0, 1], color='blue', label='Class 0', alpha=0.3)\n",
        "    else:\n",
        "        print(\"Warning: X_np has less than 2 columns. Cannot plot features X1 and X2.\")\n",
        "        # Optionally, you could break or plot a different way if needed\n",
        "        continue\n",
        "\n",
        "\n",
        "    # Plot minority class points based on zone (using the first two columns of the modified X_np)\n",
        "    minority_indices = np.where(y == 1)[0]\n",
        "    for i, zone in enumerate(zones):\n",
        "        # Access the point using NumPy indexing from X_np\n",
        "        minority_point = X_np[minority_indices[i]]\n",
        "        # Ensure the minority point has at least two elements before plotting\n",
        "        if len(minority_point) >= 2:\n",
        "            if zone == \"safe\":\n",
        "                plt.scatter(minority_point[0], minority_point[1], color='green', label='Minority - Safe', alpha=0.8, edgecolors='black')\n",
        "            elif zone == \"dangerous\":\n",
        "                plt.scatter(minority_point[0], minority_point[1], color='orange', label='Minority - Dangerous', alpha=0.8, edgecolors='black')\n",
        "            else: # noise\n",
        "                plt.scatter(minority_point[0], minority_point[1], color='red', label='Minority - Noise', alpha=0.8, edgecolors='black')\n",
        "        else:\n",
        "            print(f\"Warning: Minority point at index {minority_indices[i]} has less than 2 features. Cannot plot features X1 and X2.\")\n",
        "\n",
        "\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "\n",
        "    # Add text annotations for counts at the bottom right outside the plot\n",
        "    safe_count = zones.count(\"safe\")\n",
        "    dangerous_count = zones.count(\"dangerous\")\n",
        "    noise_count = zones.count(\"noise\")\n",
        "\n",
        "    plt.text(1.15, 0.08, f'Safe: {safe_count}', transform=plt.gca().transAxes, fontsize=10, verticalalignment='bottom')\n",
        "    plt.text(1.15, 0.04, f'Dangerous: {dangerous_count}', transform=plt.gca().transAxes, fontsize=10, verticalalignment='bottom')\n",
        "    plt.text(1.15, 0.00, f'Noise: {noise_count}', transform=plt.gca().transAxes, fontsize=10, verticalalignment='bottom')\n",
        "\n",
        "\n",
        "    plt.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(1.05, 1), loc='upper left') # Move legend outside\n",
        "\n",
        "\n",
        "    plt.title(f\"Scatter Plot with Minority Zones for k = {k}\")\n",
        "    plt.xlabel(\"Feature X1\")\n",
        "    plt.ylabel(\"Feature X2\")\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for text on the right\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VEMc4c3ICZfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79808773"
      },
      "source": [
        "### Splitting Data into Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "p-h5arhsCaFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a771783d"
      },
      "source": [
        "### Applying Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_test=sc.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "6e8M-DGaCar_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23e89acb"
      },
      "source": [
        "### Training MLP Model and Getting Minority Confidence Scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Train an MLP model on the training data\n",
        "mlp_model = MLPClassifier(random_state=42, max_iter=1000)\n",
        "mlp_model.fit(X_train, y_train)\n",
        "\n",
        "# Get confidence scores (probabilities) for the training data\n",
        "confidence_scores = mlp_model.predict_proba(X_train)"
      ],
      "metadata": {
        "id": "jRdvBXmhCclj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Minority Confidence Scores"
      ],
      "metadata": {
        "id": "WuxSxrhwE3Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The confidence score for the minority class (class 1) is in the second column\n",
        "minority_confidence = confidence_scores[:, 1]\n",
        "\n",
        "# Display the first few confidence scores\n",
        "print(\"First 10 minority confidence scores:\", minority_confidence[:10])"
      ],
      "metadata": {
        "id": "BB_eSRqGFAAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6850ca7"
      },
      "source": [
        "### Extracting Majority Confidence Scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The confidence score for the majority class (class 0) is in the first column\n",
        "majority_confidence = confidence_scores[:, 0]\n",
        "\n",
        "# Display the first few majority confidence scores\n",
        "print(\"First 10 majority confidence scores:\", majority_confidence[:10])"
      ],
      "metadata": {
        "id": "O7t_7FiCCdNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0913c93"
      },
      "source": [
        "### Classifying Minority Training Points into Zones"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify minority points in the training data by zone (k=4)\n",
        "minority_train_indices = np.where(y_train == 1)[0]\n",
        "zones_train_k4 = []\n",
        "k_for_zones = 4 # Use k=4 for zone classification\n",
        "\n",
        "# Convert X_train to a NumPy array for consistent indexing\n",
        "# X_train_np = X_train.values # Removed .values\n",
        "X_train_np = X_train # X_train is already a numpy array after scaling\n",
        "\n",
        "for i in minority_train_indices:\n",
        "    # Access the row by integer position using NumPy indexing from the NumPy array X_train_np\n",
        "    minority_point = X_train_np[i]\n",
        "    zone = classify_minority_point_zone(minority_point, X_train_np, y_train, k_for_zones) # Pass X_train_np as a numpy array\n",
        "    zones_train_k4.append(zone)\n",
        "\n",
        "# Display the first few zone classifications\n",
        "print(\"First 10 minority training points zone classifications:\", zones_train_k4[:10])"
      ],
      "metadata": {
        "id": "SixuMHnKCfiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bfd2e42"
      },
      "source": [
        "### Classifying Majority Training Points into Zones"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify majority points in the training data by zone (k=4)\n",
        "majority_train_indices = np.where(y_train == 0)[0]\n",
        "zones_majority_train_k4 = []\n",
        "k_for_zones = 4 # Use k=4 for zone classification\n",
        "\n",
        "# Convert X_train to a NumPy array for consistent indexing\n",
        "X_train_np = X_train # X_train is already a numpy array after scaling\n",
        "\n",
        "for i in majority_train_indices:\n",
        "    # Access the row by integer position using NumPy indexing from the NumPy array X_train_np\n",
        "    majority_point = X_train_np[i]\n",
        "    zone = classify_majority_point_zone(majority_point, X_train_np, y_train, k_for_zones) # Pass X_train_np as a numpy array\n",
        "    zones_majority_train_k4.append(zone)\n",
        "\n",
        "# Display the first few zone classifications\n",
        "print(\"First 10 majority training points zone classifications:\", zones_majority_train_k4[:10])"
      ],
      "metadata": {
        "id": "jMfJc2GOCgIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56ace82e"
      },
      "source": [
        "### Creating DataFrame for Minority Points Info"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to easily manage minority points, zones, and confidence\n",
        "# Select minority rows from X_train directly using a boolean mask\n",
        "minority_train_data = X_train[y_train == 1]\n",
        "minority_train_indices_in_train = np.where(y_train == 1)[0] # Get indices relative to X_train\n",
        "\n",
        "# Ensure zones_train_k4 is available (from previous step)\n",
        "# If the previous cell was not run, you might need to run it here or ensure zones_train_k4 is populated\n",
        "# For demonstration, let's assume zones_train_k4 is available\n",
        "\n",
        "# Ensure the length of zones_train_k4 matches the number of minority training points\n",
        "if len(zones_train_k4) != len(minority_train_data):\n",
        "    print(\"Mismatch between number of minority training points and zone classifications. Rerun the previous cell.\")\n",
        "else:\n",
        "    minority_info_train = pd.DataFrame({\n",
        "        # Use the integer indices from where the minority points were located in X_train\n",
        "        'Original_Train_Index': minority_train_indices_in_train,\n",
        "        'Zone': zones_train_k4,\n",
        "        'Confidence': minority_confidence[minority_train_indices_in_train], # Use indices relative to y_train\n",
        "        'Feature_X1': minority_train_data[:, 0], # Use NumPy indexing for column selection\n",
        "        'Feature_X2': minority_train_data[:, 1]  # Use NumPy indexing for column selection\n",
        "    })\n",
        "\n",
        "    # Display the first few rows of the combined information\n",
        "    display(minority_info_train.head())"
      ],
      "metadata": {
        "id": "dNkA-9qiCjOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb9875b8"
      },
      "source": [
        "### Creating DataFrame for Majority Points Info"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to easily manage majority points, zones, and confidence\n",
        "# Select majority rows from X_train directly using a boolean mask\n",
        "majority_train_data = X_train[y_train == 0]\n",
        "majority_train_indices_in_train = np.where(y_train == 0)[0] # Get indices relative to X_train\n",
        "\n",
        "# Ensure zones_majority_train_k4 is available (from previous step)\n",
        "# If the previous cell was not run, you might need to run it here or ensure zones_majority_train_k4 is populated\n",
        "# For demonstration, let's assume zones_majority_train_k4 is available\n",
        "\n",
        "# Ensure the length of zones_majority_train_k4 matches the number of majority training points\n",
        "if len(zones_majority_train_k4) != len(majority_train_data):\n",
        "    print(\"Mismatch between number of majority training points and zone classifications. Rerun the previous cell.\")\n",
        "else:\n",
        "    majority_info_train = pd.DataFrame({\n",
        "        # Use the integer indices from where the majority points were located in X_train\n",
        "        'Original_Train_Index': majority_train_indices_in_train,\n",
        "        'Zone': zones_majority_train_k4,\n",
        "        'Confidence': majority_confidence[majority_train_indices_in_train], # Use indices relative to y_train\n",
        "        'Feature_X1': majority_train_data[:, 0], # Use NumPy indexing for column selection by integer position\n",
        "        'Feature_X2': majority_train_data[:, 1]  # Use NumPy indexing for column selection by integer position\n",
        "    })\n",
        "\n",
        "    # Display the first few rows of the combined information\n",
        "    display(majority_info_train.head())"
      ],
      "metadata": {
        "id": "NnY7ueUTCjuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0827cdd4"
      },
      "source": [
        "### Defining Probability Threshold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probability_threshold = 0.7"
      ],
      "metadata": {
        "id": "LtJyN4xvCkRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6acd8da"
      },
      "source": [
        "### Calculating Weighted Average Confidence Ratios and Combining Information"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "import numpy as np # Added import\n",
        "import pandas as pd # Added import\n",
        "\n",
        "def calculate_weighted_average_confidence_ratio(point_index, X, y, k, confidence_scores):\n",
        "    \"\"\"\n",
        "    Calculates the weighted average confidence score ratio for a given point\n",
        "    based on its k-nearest neighbors' weighted average confidence scores and distances.\n",
        "\n",
        "    Args:\n",
        "        point_index (int): The index of the point in the dataset X.\n",
        "        X (np.ndarray): The complete dataset.\n",
        "        y (np.ndarray): The corresponding labels.\n",
        "        k (int): The number of nearest neighbors to consider.\n",
        "        confidence_scores (np.ndarray): Array of confidence scores (minority class probability) for each point.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - weighted_average_majority_confidence (float): Weighted average majority neighbor confidence score.\n",
        "            - weighted_average_minority_confidence (float): Weighted average minority neighbor confidence score.\n",
        "            - ratio (float): Weighted average confidence score ratio (or np.inf if weighted average minority confidence is zero).\n",
        "    \"\"\"\n",
        "    point = X[point_index]\n",
        "    distances = np.array([euclidean(point, x) for x in X])\n",
        "\n",
        "    # Get indices of sorted distances, excluding the point itself\n",
        "    nearest_indices = np.argsort(distances)[1:k+1]\n",
        "\n",
        "    weighted_majority_confidence_sum = 0.0\n",
        "    sum_majority_weights = 0.0\n",
        "    weighted_minority_confidence_sum = 0.0\n",
        "    sum_minority_weights = 0.0\n",
        "    epsilon = 1e-8 # Small value to avoid division by zero for inverse distance\n",
        "\n",
        "    for neighbor_index in nearest_indices:\n",
        "        neighbor_class = y[neighbor_index]\n",
        "        neighbor_confidence = confidence_scores[neighbor_index, 1] # Confidence for minority class\n",
        "        distance = distances[neighbor_index]\n",
        "\n",
        "        # Use inverse distance as weight\n",
        "        weight = 1 / (distance + epsilon)\n",
        "\n",
        "        if neighbor_class == 0: # Majority class\n",
        "            # For majority neighbors, we're interested in their confidence in the MAJORITY class (1 - minority confidence)\n",
        "            weighted_majority_confidence_sum += weight * (1 - neighbor_confidence)\n",
        "            sum_majority_weights += weight\n",
        "        else: # Minority class\n",
        "            # For minority neighbors, we're interested in their confidence in the MINORITY class\n",
        "            weighted_minority_confidence_sum += weight * neighbor_confidence\n",
        "            sum_minority_weights += weight\n",
        "\n",
        "    # Calculate weighted averages, handling division by zero for sums of weights\n",
        "    weighted_average_majority_confidence = weighted_majority_confidence_sum / (sum_majority_weights + epsilon)\n",
        "    weighted_average_minority_confidence = weighted_minority_confidence_sum / (sum_minority_weights + epsilon)\n",
        "\n",
        "\n",
        "    # Calculate the ratio of weighted averages, handling division by zero\n",
        "    if weighted_average_minority_confidence == 0:\n",
        "        ratio = np.inf\n",
        "    else:\n",
        "        ratio = weighted_average_majority_confidence / weighted_average_minority_confidence\n",
        "\n",
        "    return weighted_average_majority_confidence, weighted_average_minority_confidence, ratio\n",
        "\n",
        "# Calculate weighted average confidence ratios for all training points\n",
        "all_train_indices = np.arange(len(X_train))\n",
        "weighted_avg_ratios_train = []\n",
        "weighted_avg_majority_confidences = []\n",
        "weighted_avg_minority_confidences = []\n",
        "\n",
        "\n",
        "# Need confidence scores for all training points\n",
        "all_train_confidence_scores = mlp_model.predict_proba(X_train)\n",
        "\n",
        "# Convert X_train to a NumPy array for use in the calculate_weighted_average_confidence_ratio function\n",
        "# X_train_np = X_train.values # Removed .values\n",
        "X_train_np = X_train\n",
        "\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    # Pass the NumPy array X_train_np to the classification function\n",
        "    maj_avg_conf, min_avg_conf, ratio = calculate_weighted_average_confidence_ratio(i, X_train_np, y_train, k_for_zones, all_train_confidence_scores)\n",
        "    weighted_avg_majority_confidences.append(maj_avg_conf)\n",
        "    weighted_avg_minority_confidences.append(min_avg_conf)\n",
        "    weighted_avg_ratios_train.append(ratio)\n",
        "\n",
        "# Combine all information into a DataFrame\n",
        "# Ensure zones_train_k4 and zones_majority_train_k4 are correctly ordered and combined\n",
        "# The zones were calculated for minority and majority points separately, need to merge them based on original index\n",
        "\n",
        "# Create a mapping from original train index to zone\n",
        "zone_mapping = {}\n",
        "minority_train_indices = np.where(y_train == 1)[0]\n",
        "majority_train_indices = np.where(y_train == 0)[0]\n",
        "\n",
        "# Assuming zones_train_k4 corresponds to minority_train_indices\n",
        "for original_idx, zone in zip(minority_train_indices, zones_train_k4):\n",
        "    zone_mapping[original_idx] = zone\n",
        "\n",
        "# Assuming zones_majority_train_k4 corresponds to majority_train_indices\n",
        "for original_idx, zone in zip(majority_train_indices, zones_majority_train_k4):\n",
        "     zone_mapping[original_idx] = zone\n",
        "\n",
        "# Get zones in the order of all_train_indices\n",
        "all_zones_train = [zone_mapping[i] for i in all_train_indices]\n",
        "\n",
        "# Get individual confidence scores in the order of all_train_indices\n",
        "all_individual_confidence = all_train_confidence_scores[:, 1] # Confidence for minority class\n",
        "\n",
        "# Add a column for the original class label (Majority or Minority)\n",
        "original_classes = ['Minority' if label == 1 else 'Majority' for label in y_train[all_train_indices]]\n",
        "\n",
        "train_points_info_updated = pd.DataFrame({\n",
        "    'Original_Train_Index': all_train_indices,\n",
        "    'Original_Class': original_classes, # New column for class label\n",
        "    'Zone': all_zones_train,\n",
        "    'Individual_Confidence_Minority': all_individual_confidence,\n",
        "    'Weighted_Average_Majority_Neighbor_Confidence': weighted_avg_majority_confidences,\n",
        "    'Weighted_Average_Minority_Neighbor_Confidence': weighted_avg_minority_confidences,\n",
        "    'Weighted_Average_Confidence_Ratio': weighted_avg_ratios_train\n",
        "})\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "display(train_points_info_updated.head())"
      ],
      "metadata": {
        "id": "jjxQ2VqYCl90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6df899d"
      },
      "source": [
        "### Applying Filtering Rules and Creating Filtered Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import euclidean # Added import\n",
        "import numpy as np # Added import\n",
        "import pandas as pd # Added import\n",
        "\n",
        "# Define the probability threshold and calculate the ratio threshold\n",
        "probability_threshold = 0.55 # Ensure threshold is defined\n",
        "ratio_threshold = probability_threshold / (1 - probability_threshold)\n",
        "print(f\"Using probability threshold: {probability_threshold}\")\n",
        "print(f\"Calculated ratio threshold: {ratio_threshold}\")\n",
        "\n",
        "# Ensure k_for_zones is defined\n",
        "k_for_zones = 4 # Ensure k_for_zones is defined\n",
        "\n",
        "\n",
        "# Initialize sets to store indices of points to be eliminated by each rule\n",
        "# elimination_indices_rule1 = set() # Removed Rule 1\n",
        "elimination_indices_rule2 = set()\n",
        "elimination_indices_rule3 = set() # Keep the set for clarity, though it won't be populated by Rule 3 logic\n",
        "# elimination_indices_rule4 = set() # Removed Rule 4\n",
        "elimination_indices_rule5 = set()\n",
        "elimination_indices_rule6 = set() # New set for the new rule\n",
        "\n",
        "# Initialize counters for eliminated points per rule\n",
        "# eliminated_majority_rule1_count = 0 # Removed Rule 1\n",
        "# eliminated_minority_rule1_count = 0 # Removed Rule 1\n",
        "\n",
        "eliminated_majority_rule2_count = 0\n",
        "eliminated_minority_rule2_count = 0 # Rule 2 eliminates majority points\n",
        "\n",
        "eliminated_majority_rule3_count = 0 # Keep the counter, though it won't be incremented by Rule 3 logic\n",
        "eliminated_minority_rule3_count = 0 # Rule 3 eliminates dangerous majority and sometimes majority neighbors\n",
        "\n",
        "# eliminated_majority_rule4_count = 0 # Removed Rule 4\n",
        "# eliminated_minority_rule4_count = 0 # Removed Rule 4\n",
        "\n",
        "eliminated_majority_rule5_count = 0 # Rule 5 eliminates dangerous minority points\n",
        "eliminated_minority_rule5_count = 0\n",
        "\n",
        "eliminated_majority_rule6_count = 0 # Counters for the new rule\n",
        "eliminated_minority_rule6_count = 0\n",
        "\n",
        "\n",
        "# Iterate through each point in the training data\n",
        "for index, row in train_points_info_updated.iterrows():\n",
        "    original_train_index = row['Original_Train_Index']\n",
        "    original_class = row['Original_Class']\n",
        "    zone = row['Zone']\n",
        "    individual_confidence_minority = row['Individual_Confidence_Minority']\n",
        "    weighted_avg_confidence_ratio = row['Weighted_Average_Confidence_Ratio']\n",
        "\n",
        "    # Calculate individual confidence for Majority class\n",
        "    individual_confidence_majority = 1 - individual_confidence_minority\n",
        "\n",
        "    # # Apply Filtering Rule 1 (Noise Minority - Eliminate Majority Neighbors) - REMOVED\n",
        "    # if original_class == 'Minority' and zone == 'noise':\n",
        "    #     # This rule targets noise minority points with LOW majority confidence\n",
        "    #     if individual_confidence_majority < probability_threshold:\n",
        "    #         # Find k nearest neighbors in X_train for this noise minority point\n",
        "    #         point = X_train[original_train_index]\n",
        "    #         distances = np.array([euclidean(point, x) for x in X_train])\n",
        "    #         # Get indices of sorted distances, excluding the point itself\n",
        "    #         nearest_indices = np.argsort(distances)[1:k_for_zones+1]\n",
        "\n",
        "    #         # Identify Majority neighbors among the k nearest neighbors\n",
        "    #         majority_neighbors_indices = [\n",
        "    #             neighbor_index for neighbor_index in nearest_indices\n",
        "    #             if y_train[neighbor_index] == 0\n",
        "    #         ]\n",
        "    #         # Add Majority neighbors to the elimination set for Rule 1\n",
        "    #         for idx in majority_neighbors_indices:\n",
        "    #              if idx not in elimination_indices_rule1:\n",
        "    #                  elimination_indices_rule1.add(idx)\n",
        "    #                  eliminated_majority_rule1_count += 1\n",
        "\n",
        "\n",
        "    # Apply Filtering Rule 2 (Noise Majority)\n",
        "    if original_class == 'Majority' and zone == 'noise':\n",
        "        # This rule targets noise majority points with LOW majority confidence\n",
        "        if individual_confidence_majority < (1 - probability_threshold): # Check confidence in majority class\n",
        "            # Add the noise majority point to the elimination set for Rule 2\n",
        "            if original_train_index not in elimination_indices_rule2:\n",
        "                elimination_indices_rule2.add(original_train_index)\n",
        "                eliminated_majority_rule2_count += 1\n",
        "\n",
        "\n",
        "    # Apply Filtering Rule 3 (Dangerous Majority)\n",
        "    if original_class == 'Majority' and zone == 'dangerous':\n",
        "        # This rule targets dangerous majority points with LOW majority confidence\n",
        "        # MODIFIED: Only apply if BOTH conditions are met and only eliminate the dangerous majority point\n",
        "        # MODIFIED CONDITION: individual_confidence_majority < ((1 - probability_threshold) / 2)\n",
        "        if individual_confidence_majority < ((1 - probability_threshold) / 2) and weighted_avg_confidence_ratio < ratio_threshold:\n",
        "            # Add the dangerous majority point to the elimination set for Rule 3\n",
        "            if original_train_index not in elimination_indices_rule3:\n",
        "                elimination_indices_rule3.add(original_train_index)\n",
        "                eliminated_majority_rule3_count += 1\n",
        "\n",
        "            # # Find k nearest neighbors in X_train for this dangerous majority point - REMOVED NEIGHBOR ELIMINATION\n",
        "            # point = X_train[original_train_index]\n",
        "            # distances = np.array([euclidean(point, x) for x in X_train])\n",
        "            # # Get indices of sorted distances, excluding the point itself\n",
        "            # nearest_indices = np.argsort(distances)[1:k_for_zones+1]\n",
        "\n",
        "            # # Identify Majority neighbors among the k nearest neighbors - REMOVED NEIGHBOR ELIMINATION\n",
        "            # majority_neighbors_indices = [\n",
        "            #     neighbor_index for neighbor_index in nearest_indices\n",
        "            #     if y_train[neighbor_index] == 0\n",
        "            # ]\n",
        "            # # Add Majority neighbors to the elimination set for Rule 3 - REMOVED NEIGHBOR ELIMINATION\n",
        "            # for idx in majority_neighbors_indices:\n",
        "            #     if idx not in elimination_indices_rule3:\n",
        "            #         elimination_indices_rule3.add(idx)\n",
        "            #         eliminated_majority_rule3_count += 1\n",
        "\n",
        "            # # REMOVED: Else block for eliminating only the dangerous majority point\n",
        "            # else:\n",
        "            #     # Add only the dangerous majority point to the elimination set for Rule 3\n",
        "            #      if original_train_index not in elimination_indices_rule3:\n",
        "            #         elimination_indices_rule3.add(idx)\n",
        "            #         eliminated_majority_rule3_count += 1\n",
        "\n",
        "\n",
        "    # Apply Filtering Rule 5 (Dangerous Minority - Eliminate Dangerous Minority Point)\n",
        "    if original_class == 'Minority' and zone == 'dangerous':\n",
        "        # This rule targets dangerous minority points with LOW minority confidence\n",
        "        # MODIFIED CONDITION: individual_confidence_minority < (1 - probability_threshold) / 2\n",
        "        if individual_confidence_minority < (1 - probability_threshold) / 2: # Check confidence in minority class\n",
        "             if weighted_avg_confidence_ratio > ratio_threshold:\n",
        "                # Add the dangerous minority point to the elimination set for Rule 5\n",
        "                if original_train_index not in elimination_indices_rule5:\n",
        "                    elimination_indices_rule5.add(original_train_index)\n",
        "                    eliminated_minority_rule5_count += 1\n",
        "\n",
        "    # Apply Filtering Rule 6 (Noise Minority - Eliminate Noise Minority Point) - NEW RULE\n",
        "    if original_class == 'Minority' and zone == 'noise':\n",
        "        # This rule targets noise minority points with HIGH majority confidence\n",
        "        # MODIFIED CONDITION: individual_confidence_majority > (probability_threshold + ((1 - probability_threshold) / 2))\n",
        "        if individual_confidence_majority > (probability_threshold + ((1 - probability_threshold) / 2)): # Check confidence in majority class\n",
        "            # Add the noise minority point to the elimination set for Rule 6\n",
        "            if original_train_index not in elimination_indices_rule6:\n",
        "                elimination_indices_rule6.add(original_train_index)\n",
        "                eliminated_minority_rule6_count += 1\n",
        "\n",
        "\n",
        "# Combine all eliminated indices into a single set to get unique indices\n",
        "# Exclude elimination_indices_rule3 from the total elimination set\n",
        "elimination_indices_total = (\n",
        "    # elimination_indices_rule1 | # Removed Rule 1\n",
        "    elimination_indices_rule2 |\n",
        "    elimination_indices_rule3 | # Include Rule 3 in total again\n",
        "    elimination_indices_rule5 |\n",
        "    elimination_indices_rule6 # Include the new rule's elimination set\n",
        ")\n",
        "\n",
        "# Convert the set of indices to a sorted list\n",
        "elimination_indices_list = sorted(list(elimination_indices_total))\n",
        "\n",
        "print(f\"\\n--- Elimination Summary by Rule ---\") # Updated message\n",
        "# print(f\"Rule 1 (Noise Minority, Low Majority Conf): Eliminated {eliminated_majority_rule1_count} Majority neighbors\") # Removed Rule 1\n",
        "print(f\"Rule 2 (Noise Majority, Low Majority Conf): Eliminated {eliminated_majority_rule2_count} Majority points\")\n",
        "print(f\"Rule 3 (Dangerous Majority, Very Low Majority Conf & Low Ratio): Eliminated {eliminated_majority_rule3_count} Majority points\") # Updated message\n",
        "# print(f\"Rule 4 (Dangerous Minority, High Minority Conf): Eliminated {eliminated_majority_rule4_count} Majority neighbors\") # Removed Rule 4\n",
        "print(f\"Rule 5 (Dangerous Minority, Very Low Minority Conf & High Ratio): Eliminated {eliminated_minority_rule5_count} Minority points\") # Updated description\n",
        "print(f\"Rule 6 (Noise Minority, Very High Majority Conf): Eliminated {eliminated_minority_rule6_count} Minority points\") # Print count for the new rule and updated description\n",
        "\n",
        "\n",
        "print(f\"\\nTotal unique points identified for elimination: {len(elimination_indices_list)}\")\n",
        "print(f\"Indices of total unique points to be eliminated (first 20): {elimination_indices_list[:20]}...\")\n",
        "\n",
        "\n",
        "# Create the filtered dataset\n",
        "# Use a boolean mask to select points NOT in the total elimination list\n",
        "all_train_indices = np.arange(len(X_train))\n",
        "keep_mask = np.isin(all_train_indices, elimination_indices_list, invert=True)\n",
        "\n",
        "X_filtered = X_train[keep_mask]\n",
        "y_filtered = y_train[keep_mask]\n",
        "\n",
        "print(f\"\\nOriginal training data shape: {X_train.shape}\")\n",
        "print(f\"Filtered training data shape: {X_filtered.shape}\")\n",
        "\n",
        "# Display the class distribution of the filtered data\n",
        "unique, counts = np.unique(y_filtered, return_counts=True)\n",
        "filtered_class_distribution = dict(zip(unique, counts))\n",
        "print(f\"Filtered training data class distribution: {filtered_class_distribution}\")\n",
        "\n",
        "# Calculate the number of eliminated points for each class from the total filtered dataset\n",
        "unique_original, counts_original = np.unique(y_train, return_counts=True)\n",
        "original_class_distribution = dict(zip(unique_original, counts_original))\n",
        "\n",
        "eliminated_majority_total = original_class_distribution.get(0.0, 0) - filtered_class_distribution.get(0.0, 0)\n",
        "eliminated_minority_total = original_class_distribution.get(1.0, 0) - filtered_class_distribution.get(1.0, 0)\n",
        "\n",
        "print(f\"\\nTotal Majority points eliminated (unique): {eliminated_majority_total}\")\n",
        "print(f\"Total Minority points eliminated (unique): {eliminated_minority_total}\")"
      ],
      "metadata": {
        "id": "lkS0VhhFCmls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bf7bc23"
      },
      "source": [
        "### Plotting Original Training Data with Eliminated Points"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Convert X_train to a NumPy array for consistent indexing\n",
        "# X_train_np = X_train.values\n",
        "X_train_np = X_train # X_train is already a numpy array after scaling\n",
        "\n",
        "# Plot original Majority training points\n",
        "plt.scatter(X_train_np[y_train == 0, 0], X_train_np[y_train == 0, 1], color='blue', alpha=0.3, label='Original Majority')\n",
        "\n",
        "# Plot original Minority training points\n",
        "plt.scatter(X_train_np[y_train == 1, 0], X_train_np[y_train == 1, 1], color='red', alpha=0.8, label='Original Minority')\n",
        "\n",
        "# Highlight the eliminated Majority points\n",
        "eliminated_majority_indices = [i for i in elimination_indices_list if y_train[i] == 0]\n",
        "eliminated_majority_X = X_train_np[eliminated_majority_indices]\n",
        "eliminated_majority_y = y_train[eliminated_majority_indices]\n",
        "plt.scatter(eliminated_majority_X[:, 0], eliminated_majority_X[:, 1], color='black', marker='X', s=100, label='Eliminated Majority', edgecolors='white', linewidth=1.5) # Use black 'X' for eliminated Majority\n",
        "\n",
        "# Highlight the eliminated Minority points\n",
        "eliminated_minority_indices = [i for i in elimination_indices_list if y_train[i] == 1]\n",
        "eliminated_minority_X = X_train_np[eliminated_minority_indices]\n",
        "eliminated_minority_y = y_train[eliminated_minority_indices]\n",
        "plt.scatter(eliminated_minority_X[:, 0], eliminated_minority_X[:, 1], color='green', marker='X', s=100, label='Eliminated Minority', edgecolors='black', linewidth=1.5) # Use green 'X' for eliminated Minority\n",
        "\n",
        "\n",
        "plt.title(\"Original Training Data with Eliminated Points Highlighted by Class\")\n",
        "plt.xlabel(\"Feature X1\")\n",
        "plt.ylabel(\"Feature X2\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # Move legend outside\n",
        "# plt.grid(True) # Removed grid lines\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend on the right\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xBTPVPzmCnQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe64c5f5"
      },
      "source": [
        "### Plotting Filtered Training Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot filtered Majority points\n",
        "plt.scatter(X_filtered[y_filtered == 0, 0], X_filtered[y_filtered == 0, 1], color='blue', alpha=0.6, label='Filtered Majority')\n",
        "\n",
        "# Plot filtered Minority points\n",
        "plt.scatter(X_filtered[y_filtered == 1, 0], X_filtered[y_filtered == 1, 1], color='red', alpha=0.8, label='Filtered Minority')\n",
        "\n",
        "plt.title(\"Filtered Training Data Points\")\n",
        "plt.xlabel(\"Feature X1\")\n",
        "plt.ylabel(\"Feature X2\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # Move legend outside\n",
        "# plt.grid(True) # Removed grid lines\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend on the right\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "orOwm6fQCpTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ad9387"
      },
      "source": [
        "### Recalculating Zone Classifications and Confidence for Filtered Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply zone classification to the filtered training data (X_filtered, y_filtered)\n",
        "# We need to classify both minority and majority points in the filtered dataset\n",
        "\n",
        "# Get the original training indices for the filtered data points\n",
        "# This requires accessing the indices that were NOT in the elimination list\n",
        "all_train_indices = np.arange(len(X_train))\n",
        "# The keep_mask was generated in the filtering step (cell bc4d7e46)\n",
        "# Assuming keep_mask is still available from previous execution\n",
        "# If not, we would need to regenerate it or access the eliminated indices list again\n",
        "keep_mask = np.isin(all_train_indices, elimination_indices_list, invert=True)\n",
        "original_indices_filtered = all_train_indices[keep_mask]\n",
        "\n",
        "\n",
        "zones_filtered = []\n",
        "k_for_zones_filtered = 4 # Use the same k for zone classification as before\n",
        "\n",
        "# Convert X_filtered to a NumPy array for use in classification functions\n",
        "# X_filtered_np = X_filtered.values # Removed .values\n",
        "X_filtered_np = X_filtered\n",
        "\n",
        "\n",
        "# Classify minority points in the filtered data\n",
        "minority_filtered_indices_in_filtered = np.where(y_filtered == 1)[0]\n",
        "for i in minority_filtered_indices_in_filtered:\n",
        "    # Access the row by integer position using NumPy indexing from the numpy array X_filtered\n",
        "    minority_point = X_filtered[i]\n",
        "    # We need to use the *filtered* data (X_filtered_np, y_filtered) for KNN calculation\n",
        "    zone = classify_minority_point_zone(minority_point, X_filtered_np, y_filtered, k_for_zones_filtered)\n",
        "    # Get the corresponding original training index\n",
        "    original_idx = original_indices_filtered[i]\n",
        "    zones_filtered.append({'Filtered_Index': i, 'Original_Train_Index': original_idx, 'Original_Class': 'Minority', 'Zone': zone})\n",
        "\n",
        "\n",
        "# Classify majority points in the filtered data\n",
        "majority_filtered_indices_in_filtered = np.where(y_filtered == 0)[0]\n",
        "for i in majority_filtered_indices_in_filtered:\n",
        "    # Access the row by integer position using NumPy indexing from the numpy array X_filtered\n",
        "    majority_point = X_filtered[i]\n",
        "     # We need to use the *filtered* data (X_filtered_np, y_filtered) for KNN calculation\n",
        "    zone = classify_majority_point_zone(majority_point, X_filtered_np, y_filtered, k_for_zones_filtered)\n",
        "     # Get the corresponding original training index\n",
        "    original_idx = original_indices_filtered[i]\n",
        "    zones_filtered.append({'Filtered_Index': i, 'Original_Train_Index': original_idx, 'Original_Class': 'Majority', 'Zone': zone})\n",
        "\n",
        "# Convert the results to a DataFrame for easier handling\n",
        "zones_filtered_df = pd.DataFrame(zones_filtered)\n",
        "\n",
        "# --- Recalculate confidence scores using a model trained on the filtered data ---\n",
        "\n",
        "# Train a new MLP model on the filtered training data\n",
        "mlp_model_filtered = MLPClassifier(random_state=42, max_iter=1000)\n",
        "mlp_model_filtered.fit(X_filtered, y_filtered)\n",
        "\n",
        "# Get confidence scores (probabilities) for the filtered data using the filtered model\n",
        "filtered_confidence_scores = mlp_model_filtered.predict_proba(X_filtered)\n",
        "\n",
        "# Get individual confidence scores for minority and majority classes in the filtered data\n",
        "filtered_individual_confidence_minority = filtered_confidence_scores[:, 1]\n",
        "filtered_individual_confidence_majority = filtered_confidence_scores[:, 0]\n",
        "\n",
        "# Add the new confidence scores to the zones_filtered_df DataFrame\n",
        "zones_filtered_df['Individual_Confidence_Minority'] = filtered_individual_confidence_minority\n",
        "zones_filtered_df['Individual_Confidence_Majority'] = filtered_individual_confidence_majority\n",
        "\n",
        "# --- Add Minority Subgroup column to zones_filtered_df ---\n",
        "# This information is needed later for adaptive alpha calculation\n",
        "def categorize_minority_group_filtered(row):\n",
        "    if row['Original_Class'] == 'Majority':\n",
        "        return 'Majority' # Or None, or another indicator for non-minority\n",
        "    elif row['Zone'] == 'dangerous':\n",
        "        return 'Dangerous Minority'\n",
        "    elif row['Zone'] == 'safe' and row['Individual_Confidence_Minority'] < probability_threshold:\n",
        "        return 'Safe Minority Low Conf'\n",
        "    elif row['Zone'] == 'safe' and row['Individual_Confidence_Minority'] >= probability_threshold:\n",
        "        return 'Safe Minority High Conf'\n",
        "    elif row['Zone'] == 'noise':\n",
        "         return 'Noise Minority'\n",
        "    else:\n",
        "        return 'Other Minority' # Should not happen\n",
        "\n",
        "\n",
        "zones_filtered_df['Minority_Subgroup'] = zones_filtered_df.apply(categorize_minority_group_filtered, axis=1)\n",
        "\n",
        "\n",
        "# --- Recalculate Weighted Average Confidence Ratio using the filtered confidence scores ---\n",
        "\n",
        "weighted_avg_ratios_filtered = []\n",
        "weighted_avg_majority_confidences_filtered = []\n",
        "weighted_avg_minority_confidences_filtered = []\n",
        "\n",
        "# Convert X_filtered to a NumPy array for use in the calculate_weighted_average_confidence_ratio function\n",
        "# X_filtered_np = X_filtered.values # Removed .values\n",
        "X_filtered_np = X_filtered\n",
        "\n",
        "# Iterate through each point in the filtered data to calculate weighted average confidence ratio\n",
        "for i in range(len(X_filtered)):\n",
        "    # Use the calculate_weighted_average_confidence_ratio function with filtered data and filtered confidence scores\n",
        "    maj_avg_conf, min_avg_conf, ratio = calculate_weighted_average_confidence_ratio(\n",
        "        i, X_filtered_np, y_filtered, k_for_zones_filtered, filtered_confidence_scores\n",
        "    )\n",
        "    weighted_avg_majority_confidences_filtered.append(maj_avg_conf)\n",
        "    weighted_avg_minority_confidences_filtered.append(min_avg_conf)\n",
        "    weighted_avg_ratios_filtered.append(ratio)\n",
        "\n",
        "# Add the new weighted average confidence information to the zones_filtered_df DataFrame\n",
        "zones_filtered_df['Weighted_Average_Majority_Neighbor_Confidence'] = weighted_avg_majority_confidences_filtered\n",
        "zones_filtered_df['Weighted_Average_Minority_Neighbor_Confidence'] = weighted_avg_minority_confidences_filtered\n",
        "zones_filtered_df['Weighted_Average_Confidence_Ratio'] = weighted_avg_ratios_filtered\n",
        "\n",
        "\n",
        "# Rename zones_filtered_df to final_filtered_info_df to be consistent with subsequent steps\n",
        "final_filtered_info_df = zones_filtered_df\n",
        "\n",
        "# Reorder columns for clarity\n",
        "final_filtered_info_df = final_filtered_info_df[[\n",
        "    'Filtered_Index',\n",
        "    'Original_Train_Index',\n",
        "    'Original_Class',\n",
        "    'Zone',\n",
        "    'Minority_Subgroup', # Include the new column\n",
        "    'Individual_Confidence_Minority',\n",
        "    'Individual_Confidence_Majority',\n",
        "    'Weighted_Average_Majority_Neighbor_Confidence',\n",
        "    'Weighted_Average_Minority_Neighbor_Confidence',\n",
        "    'Weighted_Average_Confidence_Ratio'\n",
        "]]\n",
        "\n",
        "\n",
        "# Display the first few rows and the count of points in each zone\n",
        "display(final_filtered_info_df.head())\n",
        "\n",
        "print(\"\\nFiltered data zone distribution:\")\n",
        "print(final_filtered_info_df['Zone'].value_counts())\n",
        "\n",
        "print(\"\\nFiltered data minority subgroup distribution:\")\n",
        "print(final_filtered_info_df['Minority_Subgroup'].value_counts())\n",
        "\n",
        "\n",
        "print(\"\\nColumns in final_filtered_info_df:\")\n",
        "print(final_filtered_info_df.columns.tolist())"
      ],
      "metadata": {
        "id": "Q66qRpnPCqBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e294ae57"
      },
      "source": [
        "### Analyzing Minority Zone Counts in Filtered Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NOT REQUIRED"
      ],
      "metadata": {
        "id": "3N8Sc-VqC2hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05a46a51"
      },
      "source": [
        "### Counting Safe Minority Points with Low Confidence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NOT REQUIRED"
      ],
      "metadata": {
        "id": "wHLMQlVxC5Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1acc1629"
      },
      "source": [
        "### Allocating Synthetic Samples Based on Inverse Ratio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NOT REQUIRED"
      ],
      "metadata": {
        "id": "9QgJeK5VC64E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4a34aea"
      },
      "source": [
        "### Allocating Synthetic Samples Based on Inverse Density and Confidence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import euclidean # Import euclidean for Kth_Neighbor_Distance calculation\n",
        "\n",
        "# Ensure final_filtered_info_df is available (from cell 8d27e5ab, now updated with recalculated confidence and ratio)\n",
        "# Ensure oversample_from_safe_low_conf_count is available (total synthetic samples needed from safe low confidence, from cell nDcFANnxD9tX)\n",
        "# Ensure oversample_from_dangerous_count is available (total samples for dangerous minority, from cell nDcFANnxD9tX)\n",
        "# Note: The previous allocation split was based on 'Safe Minority < Threshold' and 'Dangerous Minority'.\n",
        "# We will now distribute the allocated samples within each of these groups based on inverse density.\n",
        "\n",
        "# --- Create safe_minority_info_df and calculate Kth_Neighbor_Distance ---\n",
        "# Filter the DataFrame to include only safe minority points with confidence < probability_threshold\n",
        "# Use the 'Individual_Confidence_Minority' which has been recalculated on the filtered data\n",
        "safe_minority_low_conf_df = final_filtered_info_df[\n",
        "    (final_filtered_info_df['Original_Class'] == 'Minority') &\n",
        "    (final_filtered_info_df['Zone'] == 'safe') &\n",
        "    (final_filtered_info_df['Individual_Confidence_Minority'] < probability_threshold)\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "\n",
        "# Calculate the distance to the k-th nearest neighbor for each point in safe_minority_low_conf_df\n",
        "k_for_density = 4 # Use the same k as for zone classification or adjust as needed\n",
        "\n",
        "kth_neighbor_distances_safe = []\n",
        "for index, row in safe_minority_low_conf_df.iterrows():\n",
        "    filtered_idx = int(row['Filtered_Index'])\n",
        "    point = X_filtered[filtered_idx] # Get the point from the filtered data using NumPy indexing\n",
        "\n",
        "    # Calculate distances to all other points in X_filtered\n",
        "    distances = np.array([euclidean(point, X_filtered[other_filtered_idx]) for other_filtered_idx in range(len(X_filtered))])\n",
        "\n",
        "\n",
        "    # Sort distances and get the k-th smallest distance (excluding the distance to itself)\n",
        "    sorted_distances = np.sort(distances)\n",
        "    # Ensure k_for_density is less than the number of points in X_filtered minus 1 (to exclude self)\n",
        "    if k_for_density < len(X_filtered):\n",
        "         # Find the index of the point itself in sorted_distances (it will be 0)\n",
        "         self_distance_index = np.where(sorted_distances == 0)[0]\n",
        "         if len(self_distance_index) > 0:\n",
        "             # If the k-th neighbor is the point itself (distance 0), take the next one\n",
        "             if k_for_density < len(sorted_distances):\n",
        "                 kth_distance = sorted_distances[k_for_density] # k-th smallest distance (0-indexed), skipping the 0 distance\n",
        "             else:\n",
        "                  # Fallback if not enough points after excluding self\n",
        "                  kth_distance = sorted_distances[-1] if len(sorted_distances) > 0 else 0.0 # Handle empty case\n",
        "         else:\n",
        "              # Should not happen for a point in the dataset, but as a fallback\n",
        "              kth_distance = sorted_distances[k_for_density -1] # k-th smallest distance (0-indexed)\n",
        "\n",
        "    else:\n",
        "         # If k is larger than or equal to the number of points, use the distance to the furthest point (excluding self)\n",
        "         # Find the distance to the furthest point, excluding the distance to itself\n",
        "         distances_excluding_self = distances[distances > 0]\n",
        "         kth_distance = np.max(distances_excluding_self) if len(distances_excluding_self) > 0 else 0.0\n",
        "\n",
        "\n",
        "    kth_neighbor_distances_safe.append(kth_distance)\n",
        "\n",
        "# Add the Kth_Neighbor_Distance as a new column to safe_minority_low_conf_df\n",
        "safe_minority_low_conf_df['Kth_Neighbor_Distance'] = kth_neighbor_distances_safe\n",
        "\n",
        "# --- Inverse Allocation Logic based on Density (using Kth_Neighbor_Distance) for Safe Minority (< Threshold) ---\n",
        "\n",
        "# Get the distances (inverse density) for safe minority points with low confidence\n",
        "distances_safe = safe_minority_low_conf_df['Kth_Neighbor_Distance'].values\n",
        "\n",
        "# Calculate allocation weights inversely proportional to distance.\n",
        "epsilon = 1e-6 # Small value to avoid division by zero\n",
        "inverse_distances_safe = 1 / (distances_safe + epsilon)\n",
        "\n",
        "# The weights for allocation should be proportional to these inverse distances\n",
        "allocation_weights_safe = inverse_distances_safe\n",
        "\n",
        "# Normalize the weights so they sum to 1\n",
        "# Handle the case where there are no safe minority points with low confidence\n",
        "if np.sum(allocation_weights_safe) > 0:\n",
        "    normalized_weights_safe = allocation_weights_safe / np.sum(allocation_weights_safe)\n",
        "else:\n",
        "    normalized_weights_safe = np.zeros_like(allocation_weights_safe) # No weights if no points\n",
        "\n",
        "\n",
        "# Get the total number of synthetic samples allocated to the 'Safe Minority < Threshold' group\n",
        "total_synthetic_from_safe_low_conf_group = oversample_from_safe_low_conf_count\n",
        "\n",
        "# Calculate the number of samples to generate from each safe minority point in this group\n",
        "num_samples_to_generate_per_point_safe = np.round(normalized_weights_safe * total_synthetic_from_safe_low_conf_group).astype(int)\n",
        "\n",
        "# Adjust for potential rounding errors to ensure the total matches\n",
        "total_generated_safe = np.sum(num_samples_to_generate_per_point_safe)\n",
        "difference_safe = total_synthetic_from_safe_low_conf_group - total_generated_safe\n",
        "\n",
        "if difference_safe != 0 and len(safe_minority_low_conf_df) > 0:\n",
        "    # Get indices of points with highest weights (in descending order)\n",
        "    highest_weight_indices_safe = np.argsort(normalized_weights_safe)[::-1]\n",
        "    for i in range(abs(difference_safe)):\n",
        "        # Use modulo to cycle through the highest weight points if difference is larger than the number of points\n",
        "        target_index_in_safe_df = highest_weight_indices_safe[i % len(highest_weight_indices_safe)]\n",
        "        if difference_safe > 0:\n",
        "            num_samples_to_generate_per_point_safe[target_index_in_safe_df] += 1\n",
        "        else:\n",
        "            # Decrement only if the current count is greater than 0 to avoid negative samples\n",
        "            if num_samples_to_generate_per_point_safe[target_index_in_safe_df] > 0:\n",
        "                 num_samples_to_generate_per_point_safe[target_index_in_safe_df] -= 1\n",
        "\n",
        "\n",
        "# Add the number of samples to generate as a new column in safe_minority_low_conf_df\n",
        "safe_minority_low_conf_df['Samples_to_Generate'] = num_samples_to_generate_per_point_safe\n",
        "\n",
        "print(f\"Total synthetic samples to generate from Safe Minority (< Threshold) group: {total_synthetic_from_safe_low_conf_group}\")\n",
        "print(f\"Total samples allocated based on density for Safe Minority (< Threshold): {np.sum(safe_minority_low_conf_df['Samples_to_Generate'])}\")\n",
        "\n",
        "\n",
        "# --- Create dangerous_minority_df and calculate Kth_Neighbor_Distance ---\n",
        "# Filter the DataFrame to include only dangerous minority points\n",
        "dangerous_minority_df = final_filtered_info_df[\n",
        "    (final_filtered_info_df['Original_Class'] == 'Minority') &\n",
        "    (final_filtered_info_df['Zone'] == 'dangerous')\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "# Calculate the distance to the k-th nearest neighbor for each point in dangerous_minority_df\n",
        "kth_neighbor_distances_dangerous = []\n",
        "for index, row in dangerous_minority_df.iterrows():\n",
        "    filtered_idx = int(row['Filtered_Index'])\n",
        "    point = X_filtered[filtered_idx] # Get the point from the filtered data using NumPy indexing\n",
        "\n",
        "    # Calculate distances to all other points in X_filtered\n",
        "    distances = np.array([euclidean(point, X_filtered[other_filtered_idx]) for other_filtered_idx in range(len(X_filtered))])\n",
        "\n",
        "\n",
        "    # Sort distances and get the k-th smallest distance (excluding the distance to itself)\n",
        "    sorted_distances = np.sort(distances)\n",
        "    # Ensure k_for_density is less than the number of points in X_filtered minus 1 (to exclude self)\n",
        "    if k_for_density < len(X_filtered):\n",
        "         # Find the index of the point itself in sorted_distances (it will be 0)\n",
        "         self_distance_index = np.where(sorted_distances == 0)[0]\n",
        "         if len(self_distance_index) > 0:\n",
        "             # If the k-th neighbor is the point itself (distance 0), take the next one\n",
        "             if k_for_density < len(sorted_distances):\n",
        "                 kth_distance = sorted_distances[k_for_density] # k-th smallest distance (0-indexed), skipping the 0 distance\n",
        "             else:\n",
        "                  # Fallback if not enough points after excluding self\n",
        "                  kth_distance = sorted_distances[-1] if len(sorted_distances) > 0 else 0.0 # Handle empty case\n",
        "\n",
        "         else:\n",
        "              # Should not happen for a point in the dataset, but as a fallback\n",
        "              kth_distance = sorted_distances[k_for_density -1] # k-th smallest distance (0-indexed)\n",
        "\n",
        "\n",
        "    else:\n",
        "         # If k is larger than or equal to the number of points, use the distance to the furthest point (excluding self)\n",
        "         # Find the distance to the furthest point, excluding the distance to itself\n",
        "         distances_excluding_self = distances[distances > 0]\n",
        "         kth_distance = np.max(distances_excluding_self) if len(distances_excluding_self) > 0 else 0.0\n",
        "\n",
        "\n",
        "    kth_neighbor_distances_dangerous.append(kth_distance)\n",
        "\n",
        "# Add the Kth_Neighbor_Distance as a new column to dangerous_minority_df\n",
        "dangerous_minority_df['Kth_Neighbor_Distance'] = kth_neighbor_distances_dangerous\n",
        "\n",
        "\n",
        "# --- Inverse Allocation Logic based on Density (using Kth_Neighbor_Distance) for Dangerous Minority ---\n",
        "\n",
        "# Get the distances (inverse density) for dangerous minority points\n",
        "distances_dangerous = dangerous_minority_df['Kth_Neighbor_Distance'].values\n",
        "\n",
        "# Calculate allocation weights inversely proportional to distance.\n",
        "inverse_distances_dangerous = 1 / (distances_dangerous + epsilon)\n",
        "\n",
        "# The weights for allocation should be proportional to these inverse distances\n",
        "allocation_weights_dangerous = inverse_distances_dangerous\n",
        "\n",
        "# Normalize the weights so they sum to 1, handling the case where there are no dangerous minority points\n",
        "if np.sum(allocation_weights_dangerous) > 0:\n",
        "    normalized_weights_dangerous = allocation_weights_dangerous / np.sum(allocation_weights_dangerous)\n",
        "else:\n",
        "    normalized_weights_dangerous = np.zeros_like(allocation_weights_dangerous) # No weights if no points\n",
        "\n",
        "\n",
        "# Get the total number of synthetic samples allocated to the 'Dangerous Minority' group\n",
        "total_synthetic_from_dangerous_group = oversample_from_dangerous_count\n",
        "\n",
        "# Calculate the number of samples to generate from each dangerous minority point\n",
        "num_samples_to_generate_per_point_dangerous = np.round(normalized_weights_dangerous * total_synthetic_from_dangerous_group).astype(int)\n",
        "\n",
        "# Adjust for potential rounding errors to ensure the total matches\n",
        "total_generated_dangerous = np.sum(num_samples_to_generate_per_point_dangerous)\n",
        "difference_dangerous = total_synthetic_from_dangerous_group - total_generated_dangerous\n",
        "\n",
        "if difference_dangerous != 0 and len(dangerous_minority_df) > 0:\n",
        "    # Get indices of points with highest weights (in descending order)\n",
        "    highest_weight_indices_dangerous = np.argsort(normalized_weights_dangerous)[::-1]\n",
        "    for i in range(abs(difference_dangerous)):\n",
        "        # Use modulo to cycle through the highest weight points if difference is larger than the number of points\n",
        "        target_index_in_dangerous_df = highest_weight_indices_dangerous[i % len(highest_weight_indices_dangerous)]\n",
        "        if difference_dangerous > 0:\n",
        "            num_samples_to_generate_per_point_dangerous[target_index_in_dangerous_df] += 1\n",
        "        else:\n",
        "             # Decrement only if the current count is greater than 0 to avoid negative samples\n",
        "            if num_samples_to_generate_per_point_dangerous[target_index_in_dangerous_df] > 0:\n",
        "                 num_samples_to_generate_per_point_dangerous[target_index_in_dangerous_df] -= 1\n",
        "\n",
        "\n",
        "# Add the number of samples to generate as a new column in dangerous_minority_df\n",
        "dangerous_minority_df['Samples_to_Generate'] = num_samples_to_generate_per_point_dangerous\n",
        "\n",
        "print(f\"Total synthetic samples to generate from Dangerous Minority group: {total_synthetic_from_dangerous_group}\")\n",
        "print(f\"Total samples allocated based on density for Dangerous Minority: {np.sum(dangerous_minority_df['Samples_to_Generate'])}\")\n",
        "\n",
        "\n",
        "print(\"\\nFirst 5 safe minority points (< Threshold) with their allocated samples to generate:\")\n",
        "display(safe_minority_low_conf_df[['Filtered_Index', 'Original_Train_Index', 'Zone', 'Kth_Neighbor_Distance', 'Samples_to_Generate']].head())\n",
        "\n",
        "print(\"\\nDistribution of samples to generate across safe minority points (< Threshold):\")\n",
        "allocation_counts_safe = safe_minority_low_conf_df['Samples_to_Generate'].value_counts()\n",
        "print(allocation_counts_safe[allocation_counts_safe > 0])\n",
        "\n",
        "print(\"\\nFirst 5 dangerous minority points with their allocated samples to generate:\")\n",
        "display(dangerous_minority_df[['Filtered_Index', 'Original_Train_Index', 'Zone', 'Kth_Neighbor_Distance', 'Samples_to_Generate']].head())\n",
        "\n",
        "print(\"\\nDistribution of samples to generate across dangerous minority points:\")\n",
        "allocation_counts_dangerous = dangerous_minority_df['Samples_to_Generate'].value_counts()\n",
        "print(allocation_counts_dangerous[allocation_counts_dangerous > 0])"
      ],
      "metadata": {
        "id": "vcIAdT1wH_bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63d869d3"
      },
      "source": [
        "### Identifying Target Minority Groups for Clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure final_filtered_info_df and probability_threshold are available from previous cells\n",
        "\n",
        "# Identify Dangerous Minority points\n",
        "dangerous_minority_points_info = final_filtered_info_df[\n",
        "    (final_filtered_info_df['Original_Class'] == 'Minority') &\n",
        "    (final_filtered_info_df['Zone'] == 'dangerous')\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "\n",
        "# Identify Safe Minority points with low confidence\n",
        "# Use the 'Individual_Confidence_Minority' which has been recalculated on the filtered data\n",
        "safe_minority_low_conf_points_info = final_filtered_info_df[\n",
        "    (final_filtered_info_df['Original_Class'] == 'Minority') &\n",
        "    (final_filtered_info_df['Zone'] == 'safe') &\n",
        "    (final_filtered_info_df['Individual_Confidence_Minority'] < probability_threshold)\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "# Identify Safe Minority points with high confidence\n",
        "# Use the 'Individual_Confidence_Minority' which has been recalculated on the filtered data\n",
        "safe_minority_high_conf_points_info = final_filtered_info_df[\n",
        "    (final_filtered_info_df['Original_Class'] == 'Minority') &\n",
        "    (final_filtered_info_df['Zone'] == 'safe') &\n",
        "    (final_filtered_info_df['Individual_Confidence_Minority'] >= probability_threshold)\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "\n",
        "# Combine the three groups of points into a single DataFrame for clustering\n",
        "# Keep relevant columns: Filtered_Index is crucial for linking back to X_filtered\n",
        "target_minority_points_info_three_groups = pd.concat(\n",
        "    [dangerous_minority_points_info, safe_minority_low_conf_points_info, safe_minority_high_conf_points_info],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Add a column to indicate which of the three groups the point belongs to\n",
        "def categorize_minority_group(row):\n",
        "    if row['Zone'] == 'dangerous':\n",
        "        return 'Dangerous Minority'\n",
        "    elif row['Zone'] == 'safe' and row['Individual_Confidence_Minority'] < probability_threshold:\n",
        "        return 'Safe Minority Low Conf'\n",
        "    elif row['Zone'] == 'safe' and row['Individual_Confidence_Minority'] >= probability_threshold:\n",
        "        return 'Safe Minority High Conf'\n",
        "    else:\n",
        "        return 'Other Minority' # Should not happen for filtered minority points\n",
        "\n",
        "\n",
        "target_minority_points_info_three_groups['Minority_Subgroup'] = target_minority_points_info_three_groups.apply(categorize_minority_group, axis=1)\n",
        "\n",
        "\n",
        "print(f\"Number of Dangerous Minority points: {len(dangerous_minority_points_info)}\")\n",
        "print(f\"Number of Safe Minority with Low Confidence points: {len(safe_minority_low_conf_points_info)}\")\n",
        "print(f\"Number of Safe Minority with High Confidence points: {len(safe_minority_high_conf_points_info)}\")\n",
        "print(f\"Total target minority points for clustering: {len(target_minority_points_info_three_groups)}\")\n",
        "\n",
        "# Display the distribution across the three subgroups\n",
        "print(\"\\nDistribution of minority points across the three target subgroups:\")\n",
        "print(target_minority_points_info_three_groups['Minority_Subgroup'].value_counts())\n",
        "\n",
        "\n",
        "# Display the first few rows of the combined target minority points info\n",
        "print(\"\\nCombined Target Minority Points Info (first 5 rows):\")\n",
        "display(target_minority_points_info_three_groups.head())"
      ],
      "metadata": {
        "id": "2ZMvwlldISbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfb08af"
      },
      "source": [
        "### Preparing Data for Clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure target_minority_points_info_three_groups and X_filtered are available from previous steps\n",
        "\n",
        "# Get the indices in X_filtered for these target minority points\n",
        "target_minority_filtered_indices_three_groups = target_minority_points_info_three_groups['Filtered_Index'].values\n",
        "\n",
        "# Select the corresponding rows from X_filtered\n",
        "X_target_minority_three_groups = X_filtered[target_minority_filtered_indices_three_groups]\n",
        "\n",
        "print(f\"Shape of X_target_minority_three_groups (features for clustering): {X_target_minority_three_groups.shape}\")"
      ],
      "metadata": {
        "id": "MmJnSh42IZKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b91fde88"
      },
      "source": [
        "### Applying Agglomerative Clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting the silhouette scores\n",
        "\n",
        "# Ensure X_target_minority_three_groups and target_minority_points_info_three_groups are available from previous steps\n",
        "\n",
        "# --- Determine Optimal Number of Clusters using Silhouette Score ---\n",
        "# We need to evaluate silhouette scores for a range of possible cluster numbers.\n",
        "# The minimum number of clusters should be 2.\n",
        "# The maximum number of clusters should be less than the number of samples.\n",
        "max_clusters = min(10, X_target_minority_three_groups.shape[0] - 1) # Evaluate up to 10 clusters or (n_samples - 1)\n",
        "\n",
        "if max_clusters < 2:\n",
        "    print(\"Not enough samples to perform clustering with at least 2 clusters.\")\n",
        "    optimal_n_clusters_three_groups = max_clusters # Fallback, though clustering won't be meaningful\n",
        "    silhouette_avg = -1 # Indicate no meaningful score\n",
        "    range_n_clusters = [] # Empty the range if not enough clusters\n",
        "else:\n",
        "    range_n_clusters = list(range(2, max_clusters + 1)) # Range from 2 to max_clusters\n",
        "\n",
        "    silhouette_scores = []\n",
        "\n",
        "    print(f\"Evaluating silhouette scores for n_clusters from 2 to {max_clusters}...\")\n",
        "\n",
        "    for n_clusters in range_n_clusters:\n",
        "        # Apply Agglomerative Clustering\n",
        "        agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
        "        cluster_labels = agg_clustering.fit_predict(X_target_minority_three_groups)\n",
        "\n",
        "        # Calculate silhouette score. Ensure there is more than one cluster and more than one sample.\n",
        "        if len(np.unique(cluster_labels)) > 1 and len(X_target_minority_three_groups) > 1:\n",
        "            score = silhouette_score(X_target_minority_three_groups, cluster_labels)\n",
        "            silhouette_scores.append(score)\n",
        "            print(f\"  n_clusters = {n_clusters}, Silhouette Score = {score:.4f}\")\n",
        "        else:\n",
        "            silhouette_scores.append(-1) # Append a low score if silhouette is not applicable\n",
        "            print(f\"  n_clusters = {n_clusters}, Silhouette Score not applicable (not enough clusters or samples)\")\n",
        "\n",
        "\n",
        "    # Find the optimal number of clusters based on the highest silhouette score\n",
        "    if silhouette_scores:\n",
        "        # Get the index of the maximum silhouette score, excluding -1 values\n",
        "        valid_scores = [score for score in silhouette_scores if score > -1]\n",
        "        if valid_scores:\n",
        "            optimal_cluster_index = silhouette_scores.index(max(valid_scores))\n",
        "            # Get the corresponding optimal n_clusters from the range\n",
        "            optimal_n_clusters_three_groups = range_n_clusters[optimal_cluster_index]\n",
        "            silhouette_avg = max(valid_scores)\n",
        "            print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_n_clusters_three_groups} (Silhouette Score: {silhouette_avg:.4f})\")\n",
        "        else:\n",
        "            # Fallback if no valid silhouette scores could be calculated\n",
        "            optimal_n_clusters_three_groups = min(5, X_target_minority_three_groups.shape[0]) # Use a default value\n",
        "            silhouette_avg = -1\n",
        "            print(f\"\\nCould not calculate valid silhouette scores, using default n_clusters = {optimal_n_clusters_three_groups}\")\n",
        "    else:\n",
        "        # Fallback if silhouette_scores list is empty\n",
        "        optimal_n_clusters_three_groups = min(5, X_target_minority_three_groups.shape[0]) # Use a default value\n",
        "        silhouette_avg = -1\n",
        "        print(f\"\\nCould not calculate any silhouette scores, using default n_clusters = {optimal_n_clusters_three_groups}\")\n",
        "\n",
        "    # Plot the silhouette scores\n",
        "    if range_n_clusters and valid_scores:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(range_n_clusters, silhouette_scores, marker='o')\n",
        "        plt.title(\"Silhouette Scores for Various Numbers of Clusters\")\n",
        "        plt.xlabel(\"Number of Clusters (n_clusters)\")\n",
        "        plt.ylabel(\"Silhouette Score\")\n",
        "        plt.xticks(range_n_clusters)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# --- Apply Agglomerative Clustering with the Optimal Number of Clusters ---\n",
        "\n",
        "# Ensure optimal_n_clusters_three_groups is at least 1\n",
        "if optimal_n_clusters_three_groups < 1:\n",
        "    optimal_n_clusters_three_groups = 1 # Cannot have 0 clusters\n",
        "\n",
        "\n",
        "# Initialize and fit Agglomerative Clustering with the optimal number\n",
        "agg_clustering_three_groups = AgglomerativeClustering(n_clusters=optimal_n_clusters_three_groups, linkage='ward')\n",
        "\n",
        "# Get cluster labels for each point in X_target_minority_three_groups\n",
        "cluster_labels_three_groups = agg_clustering_three_groups.fit_predict(X_target_minority_three_groups)\n",
        "\n",
        "# Add the cluster labels back to the target_minority_points_info_three_groups DataFrame\n",
        "# This links the cluster assignment to the information about each point\n",
        "target_minority_points_info_three_groups['Cluster_Label'] = cluster_labels_three_groups\n",
        "\n",
        "print(f\"\\nApplied Agglomerative Clustering with n_clusters = {optimal_n_clusters_three_groups} (Optimal)\")\n",
        "print(\"\\nFirst 5 target minority points with their assigned cluster labels:\")\n",
        "display(target_minority_points_info_three_groups.head())\n",
        "\n",
        "print(\"\\nDistribution of points across clusters:\")\n",
        "print(target_minority_points_info_three_groups['Cluster_Label'].value_counts().sort_index())\n",
        "\n",
        "# You might want to visualize the dendrogram to help select the number of clusters\n",
        "# This requires importing and using scipy.cluster.hierarchy\n",
        "# However, for large number of points, dendrogram can be very large.\n",
        "# We will skip dendrogram visualization for now, but you can add it if needed."
      ],
      "metadata": {
        "id": "L1If5ypsIe_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34cd139d"
      },
      "source": [
        "### Analyzing Cluster Composition and Density (Currently density is kept in no use mode)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import euclidean # Needed for distance calculation\n",
        "\n",
        "# Ensure target_minority_points_info_three_groups and X_filtered are available\n",
        "# Ensure probability_threshold is available\n",
        "\n",
        "# Calculate cluster centroids and average distances (inverse density) for each cluster\n",
        "cluster_density_info = {}\n",
        "k_for_density_calculation = 4 # Use the same k as for zone classification or adjust as needed\n",
        "\n",
        "# Get the feature data corresponding to the clustered minority points\n",
        "X_target_minority_three_groups = X_filtered[target_minority_points_info_three_groups['Filtered_Index']]\n",
        "\n",
        "# Iterate through each cluster\n",
        "for cluster_label in sorted(target_minority_points_info_three_groups['Cluster_Label'].unique()):\n",
        "    # Get points belonging to this cluster\n",
        "    points_in_cluster_info = target_minority_points_info_three_groups[\n",
        "        target_minority_points_info_three_groups['Cluster_Label'] == cluster_label\n",
        "    ]\n",
        "\n",
        "    # Get the feature data for points in this cluster\n",
        "    X_cluster = X_filtered[points_in_cluster_info['Filtered_Index']]\n",
        "\n",
        "    if len(X_cluster) > 0:\n",
        "        # Calculate cluster centroid\n",
        "        cluster_centroid = np.mean(X_cluster, axis=0)\n",
        "\n",
        "        # Calculate average distance to the cluster centroid\n",
        "        distances_to_centroid = np.array([euclidean(point, cluster_centroid) for point in X_cluster])\n",
        "        average_distance_to_centroid = np.mean(distances_to_centroid) if len(distances_to_centroid) > 0 else 0.0\n",
        "\n",
        "        # Calculate inverse average distance to centroid (as a density measure)\n",
        "        epsilon_density_centroid = 1e-8 # Small value to avoid division by zero\n",
        "        inverse_average_distance_to_centroid = 1 / (average_distance_to_centroid + epsilon_density_centroid)\n",
        "\n",
        "\n",
        "        # --- Density Calculation: Average Distance to ALL Filtered Points ---\n",
        "        # This gives a measure of density relative to the entire filtered dataset\n",
        "        avg_distances_to_all_filtered = []\n",
        "        for point_in_cluster_filtered_idx in points_in_cluster_info['Filtered_Index']:\n",
        "             point = X_filtered[point_in_cluster_filtered_idx]\n",
        "             # Calculate distances to all other points in X_filtered\n",
        "             distances = np.array([euclidean(point, other_point) for other_point in X_filtered])\n",
        "\n",
        "             # Average distance to all other points (excluding self distance)\n",
        "             # Handle cases where there are very few points\n",
        "             if len(distances) > 1:\n",
        "                  avg_distance = np.mean(distances[distances > 0])\n",
        "             elif len(distances) == 1:\n",
        "                  avg_distance = distances[0] # Distance to itself is 0, but if only one point, avg dist to others is not well-defined. Could use a large value or 0.\n",
        "             else:\n",
        "                  avg_distance = 0.0 # No points\n",
        "\n",
        "             avg_distances_to_all_filtered.append(avg_distance)\n",
        "\n",
        "        # Calculate the overall average of these average distances for the cluster\n",
        "        # Use a small epsilon to avoid division by zero if all avg_distances are 0\n",
        "        overall_avg_distance_for_cluster = np.mean(avg_distances_to_all_filtered) if len(avg_distances_to_all_filtered) > 0 else 0.0\n",
        "        epsilon_density = 1e-8\n",
        "        inverse_density_all_filtered = 1 / (overall_avg_distance_for_cluster + epsilon_density) if overall_avg_distance_for_cluster > 0 else 0.0 # Handle case where avg distance is 0\n",
        "\n",
        "\n",
        "        # Analyze cluster composition (proportions of the three subgroups)\n",
        "        total_points_in_cluster = len(points_in_cluster_info)\n",
        "        dangerous_count = points_in_cluster_info[points_in_cluster_info['Minority_Subgroup'] == 'Dangerous Minority'].shape[0]\n",
        "        safe_low_conf_count = points_in_cluster_info[points_in_cluster_info['Minority_Subgroup'] == 'Safe Minority Low Conf'].shape[0]\n",
        "        safe_high_conf_count = points_in_cluster_info[points_in_cluster_info['Minority_Subgroup'] == 'Safe Minority High Conf'].shape[0]\n",
        "\n",
        "        dangerous_proportion_in_cluster = dangerous_count / total_points_in_cluster if total_points_in_cluster > 0 else 0.0\n",
        "        safe_low_conf_proportion_in_cluster = safe_low_conf_count / total_points_in_cluster if total_points_in_cluster > 0 else 0.0\n",
        "        safe_high_conf_proportion_in_cluster = safe_high_conf_count / total_points_in_cluster if total_points_in_cluster > 0 else 0.0\n",
        "\n",
        "        cluster_density_info[cluster_label] = {\n",
        "            'Total_Points': total_points_in_cluster,\n",
        "            'Dangerous_Count': dangerous_count,\n",
        "            'Safe_Low_Conf_Count': safe_low_conf_count,\n",
        "            'Safe_High_Conf_Count': safe_high_conf_count,\n",
        "            'Dangerous_Proportion': dangerous_proportion_in_cluster,\n",
        "            'Safe_Low_Conf_Proportion': safe_low_conf_proportion_in_cluster,\n",
        "            'Safe_High_Conf_Proportion': safe_high_conf_proportion_in_cluster,\n",
        "            'Inverse_Density_All_Filtered': inverse_density_all_filtered, # Existing density metric (now the primary one for allocation)\n",
        "            'Average_Distance_to_Centroid': average_distance_to_centroid, # Still calculated, but not used for primary density in allocation\n",
        "            'Inverse_Average_Distance_to_Centroid': inverse_average_distance_to_centroid # Still calculated, but not used for primary density in allocation\n",
        "        }\n",
        "    else:\n",
        "         # Handle empty clusters if any (unlikely with Agglomerative Clustering on non-empty data)\n",
        "         cluster_density_info[cluster_label] = {\n",
        "            'Total_Points': 0,\n",
        "            'Dangerous_Count': 0,\n",
        "            'Safe_Low_Conf_Count': 0,\n",
        "            'Safe_High_Conf_Count': 0,\n",
        "            'Dangerous_Proportion': 0.0,\n",
        "            'Safe_Low_Conf_Proportion': 0.0,\n",
        "            'Safe_High_Conf_Proportion': 0.0,\n",
        "            'Inverse_Density_All_Filtered': 0.0,\n",
        "            'Average_Distance_to_Centroid': 0.0,\n",
        "            'Inverse_Average_Distance_to_Centroid': 0.0\n",
        "        }\n",
        "\n",
        "\n",
        "# Convert the dictionary to a DataFrame for easier analysis\n",
        "cluster_analysis_df = pd.DataFrame.from_dict(cluster_density_info, orient='index').reset_index().rename(columns={'index': 'Cluster_Label'})\n",
        "\n",
        "print(\"\\nCluster Composition and Density Analysis (using average distance to all filtered points as density):\") # Updated message\n",
        "display(cluster_analysis_df)"
      ],
      "metadata": {
        "id": "X7Ku9BmCIpUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40e0b7da"
      },
      "source": [
        "### Calculating Cluster Allocation Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate allocation weights based on cluster composition and inverse density\n",
        "\n",
        "# Add columns for normalized inverse density and normalized target proportion\n",
        "# Use a small epsilon to avoid division by zero during normalization\n",
        "epsilon_norm = 1e-8\n",
        "\n",
        "# Normalize Inverse Density across all clusters\n",
        "# Corrected: Use the new centroid-based inverse density column\n",
        "total_inverse_density = cluster_analysis_df['Inverse_Average_Distance_to_Centroid'].sum()\n",
        "if total_inverse_density > 0:\n",
        "    cluster_analysis_df['Normalized_Inverse_Density'] = cluster_analysis_df['Inverse_Average_Distance_to_Centroid'] / total_inverse_density\n",
        "else:\n",
        "    cluster_analysis_df['Normalized_Inverse_Density'] = 0.0\n",
        "\n",
        "# Calculate the proportion of \"harder\" minority points (Dangerous + Safe Low Conf) in each cluster\n",
        "cluster_analysis_df['Harder_Proportion_in_Cluster'] = cluster_analysis_df['Dangerous_Proportion'] + cluster_analysis_df['Safe_Low_Conf_Proportion']\n",
        "\n",
        "\n",
        "# Calculate the total number of \"harder\" minority points across all target minority points\n",
        "total_dangerous_points = target_minority_points_info_three_groups[target_minority_points_info_three_groups['Minority_Subgroup'] == 'Dangerous Minority'].shape[0]\n",
        "total_safe_low_conf_points = target_minority_points_info_three_groups[target_minority_points_info_three_groups['Minority_Subgroup'] == 'Safe Minority Low Conf'].shape[0]\n",
        "total_harder_minority_points = total_dangerous_points + total_safe_low_conf_points\n",
        "\n",
        "\n",
        "# Calculate the proportion of \"harder\" minority points in each cluster relative to the total harder points\n",
        "# This will be used for the composition-based weighting\n",
        "cluster_analysis_df['Proportion_of_Total_Harder'] = (cluster_analysis_df['Dangerous_Count'] + cluster_analysis_df['Safe_Low_Conf_Count']) / (total_harder_minority_points + epsilon_norm)\n",
        "\n",
        "\n",
        "# Calculate the combined allocation weight for each cluster based on the modified logic\n",
        "allocation_weights = []\n",
        "for index, row in cluster_analysis_df.iterrows():\n",
        "    cluster_label = row['Cluster_Label']\n",
        "    normalized_inverse_density = row['Normalized_Inverse_Density'] # Keep for potential future use or analysis, but not for allocation weight\n",
        "    dangerous_count = row['Dangerous_Count']\n",
        "    safe_low_conf_count = row['Safe_Low_Conf_Count']\n",
        "    proportion_of_total_harder = row['Proportion_of_Total_Harder']\n",
        "\n",
        "\n",
        "    # Modified Logic for Allocation Weight: Based ONLY on the proportion of \"harder\" points in the cluster\n",
        "    weight = proportion_of_total_harder\n",
        "\n",
        "    allocation_weights.append(weight)\n",
        "\n",
        "cluster_analysis_df['Combined_Allocation_Weight'] = allocation_weights\n",
        "\n",
        "# Normalize the combined allocation weights so they sum to 1\n",
        "total_combined_weight = cluster_analysis_df['Combined_Allocation_Weight'].sum()\n",
        "if total_combined_weight > 0:\n",
        "     cluster_analysis_df['Normalized_Combined_Allocation_Weight'] = cluster_analysis_df['Combined_Allocation_Weight'] / total_combined_weight\n",
        "else:\n",
        "     cluster_analysis_df['Normalized_Combined_Allocation_Weight'] = 0.0\n",
        "\n",
        "\n",
        "# Calculate the number of samples to generate from each cluster\n",
        "# Ensure synthetic_samples_needed is available\n",
        "if 'synthetic_samples_needed' not in locals():\n",
        "    print(\"Warning: 'synthetic_samples_needed' not found. Please run the cell calculating synthetic sample needs.\")\n",
        "    # For now, let's assume 0 samples to generate if synthetic_samples_needed is not defined\n",
        "    synthetic_samples_needed = 0\n",
        "\n",
        "\n",
        "cluster_analysis_df['Samples_to_Generate_in_Cluster'] = np.round(cluster_analysis_df['Normalized_Combined_Allocation_Weight'] * synthetic_samples_needed).astype(int)\n",
        "\n",
        "\n",
        "# Adjust for potential rounding errors to ensure the total matches synthetic_samples_needed\n",
        "total_generated_in_clusters = cluster_analysis_df['Samples_to_Generate_in_Cluster'].sum()\n",
        "difference_in_allocation = synthetic_samples_needed - total_generated_in_clusters\n",
        "\n",
        "if difference_in_allocation != 0 and len(cluster_analysis_df) > 0:\n",
        "    # Get indices of clusters with highest weights (in descending order)\n",
        "    highest_weight_cluster_indices = cluster_analysis_df['Normalized_Combined_Allocation_Weight'].argsort()[::-1]\n",
        "    for i in range(abs(difference_in_allocation)):\n",
        "        # Use modulo to cycle through the highest weight clusters if difference is larger than the number of clusters\n",
        "        target_index_in_cluster_df = highest_weight_cluster_indices[i % len(cluster_analysis_df)]\n",
        "        if difference_in_allocation > 0:\n",
        "            cluster_analysis_df.loc[target_index_in_cluster_df, 'Samples_to_Generate_in_Cluster'] += 1\n",
        "        else:\n",
        "             # Decrement only if the current count is greater than 0 to avoid negative samples\n",
        "            if cluster_analysis_df.loc[target_index_in_cluster_df, 'Samples_to_Generate_in_Cluster'] > 0:\n",
        "                 cluster_analysis_df.loc[target_index_in_cluster_df, 'Samples_to_Generate_in_Cluster'] -= 1\n",
        "\n",
        "\n",
        "print(\"\\nCluster Analysis with Allocation Weights and Samples to Generate:\")\n",
        "display(cluster_analysis_df)\n",
        "\n",
        "print(f\"\\nTotal synthetic samples to generate: {synthetic_samples_needed}\")\n",
        "print(f\"Total samples allocated to clusters: {cluster_analysis_df['Samples_to_Generate_in_Cluster'].sum()}\")"
      ],
      "metadata": {
        "id": "Yy1J_vonJKQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67c4985"
      },
      "source": [
        "###  Synthetic Sample Generation and obtaining of balanced data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "import numpy as np\n",
        "import pandas as pd # Import pandas to work with DataFrames if needed\n",
        "# from imblearn.under_sampling import RandomUnderSampler # Consider if a more robust undersampling method is needed later\n",
        "\n",
        "# Ensure X_filtered, y_filtered are available (filtered training data)\n",
        "# Ensure synthetic_samples_needed is available (total number of minority samples to generate)\n",
        "# Ensure final_filtered_info_df is available (contains zone and NOW updated confidence info for all filtered points)\n",
        "# Ensure probability_threshold and ratio_threshold are available\n",
        "# Ensure target_minority_points_info_three_groups is available (from Plan Step 1, contains Cluster_Label and Minority_Subgroup)\n",
        "# Ensure cluster_analysis_df is available (from Plan Step 5, contains Samples_to_Generate_in_Cluster)\n",
        "\n",
        "\n",
        "# Define k for neighbor selection during synthesis\n",
        "k_for_synthesis_neighbors = 3\n",
        "\n",
        "# Define epsilon for the alpha calculation\n",
        "epsilon = 1e-6\n",
        "\n",
        "# List to store generated synthetic samples (used only for oversampling)\n",
        "synthetic_samples = []\n",
        "synthetic_labels = [] # Should all be minority class (1)\n",
        "\n",
        "print(f\"Processing data balancing...\")\n",
        "\n",
        "# --- Conditional Balancing: Undersampling or Oversampling ---\n",
        "\n",
        "if synthetic_samples_needed < 0:\n",
        "    # --- Perform Undersampling ---\n",
        "    print(f\"Minority class ({len(y_filtered[y_filtered == 1])}) is larger than Majority class ({len(y_filtered[y_filtered == 0])}) in filtered data.\")\n",
        "    print(f\"Undersampling minority class to match majority count: {target_minority_count}\") # target_minority_count was calculated in cell nDcFANnxD9tX\n",
        "\n",
        "    # Identify minority class indices in the filtered data\n",
        "    minority_filtered_indices = np.where(y_filtered == 1)[0]\n",
        "\n",
        "    # Calculate the number of minority samples to remove\n",
        "    num_minority_to_remove = abs(synthetic_samples_needed)\n",
        "\n",
        "    # Ensure we don't try to remove more samples than exist\n",
        "    if num_minority_to_remove > len(minority_filtered_indices):\n",
        "        print(f\"Warning: Attempted to remove {num_minority_to_remove} minority samples, but only {len(minority_filtered_indices)} exist. Removing all minority samples.\")\n",
        "        num_minority_to_remove = len(minority_filtered_indices)\n",
        "\n",
        "\n",
        "    # Randomly select indices of minority samples to remove\n",
        "    indices_to_remove = np.random.choice(minority_filtered_indices, size=num_minority_to_remove, replace=False)\n",
        "\n",
        "    # Create a mask to keep the samples that are not in indices_to_remove\n",
        "    keep_mask_undersampling = np.isin(np.arange(len(X_filtered)), indices_to_remove, invert=True)\n",
        "\n",
        "    # Create the undersampled dataset\n",
        "    X_balanced_custom = X_filtered[keep_mask_undersampling]\n",
        "    y_balanced_custom = y_filtered[keep_mask_undersampling]\n",
        "\n",
        "    print(f\"Undersampling complete. New data shape: {X_balanced_custom.shape}\")\n",
        "\n",
        "\n",
        "elif synthetic_samples_needed > 0:\n",
        "    # --- Perform Oversampling (New Cluster-Based Logic) ---\n",
        "    print(f\"Minority class ({len(y_filtered[y_filtered == 1])}) is smaller than Majority class ({len(y_filtered[y_filtered == 0])}) in filtered data.\")\n",
        "    print(f\"Generating {synthetic_samples_needed} synthetic samples to balance classes using cluster-based allocation.\")\n",
        "\n",
        "    # Ensure cluster_analysis_df with 'Samples_to_Generate_in_Cluster' is available\n",
        "    if 'cluster_analysis_df' not in locals() or 'Samples_to_Generate_in_Cluster' not in cluster_analysis_df.columns:\n",
        "        print(\"Error: Cluster analysis results with sample allocation not found. Please run Plan Steps 1-5 first.\")\n",
        "    else:\n",
        "        # Iterate through each cluster based on the allocation\n",
        "        for index, cluster_row in cluster_analysis_df.iterrows():\n",
        "            cluster_label = cluster_row['Cluster_Label']\n",
        "            num_samples_to_generate_from_cluster = int(cluster_row['Samples_to_Generate_in_Cluster'])\n",
        "\n",
        "            if num_samples_to_generate_from_cluster > 0:\n",
        "                print(f\"Generating {num_samples_to_generate_from_cluster} samples from Cluster {cluster_label}...\")\n",
        "\n",
        "                # Get the points belonging to this cluster from target_minority_points_info_three_groups\n",
        "                points_in_cluster_info = target_minority_points_info_three_groups[\n",
        "                    target_minority_points_info_three_groups['Cluster_Label'] == cluster_label\n",
        "                ].copy() # Use copy to avoid SettingWithCopyWarning\n",
        "\n",
        "                # --- Calculate Kth_Neighbor_Distance (Inverse Density) for points within this cluster relative to X_filtered ---\n",
        "                # This was previously done for the dangerous and safe low conf groups separately.\n",
        "                # Now calculate it for all points in the current cluster within the context of X_filtered.\n",
        "                k_for_density_calculation = 4 # Use the same k as for zone classification\n",
        "\n",
        "                kth_neighbor_distances_cluster = []\n",
        "                for idx, point_info in points_in_cluster_info.iterrows():\n",
        "                     filtered_idx = int(point_info['Filtered_Index'])\n",
        "                     point = X_filtered[filtered_idx] # Get the point from the filtered data using NumPy indexing\n",
        "\n",
        "                     # Calculate distances to all other points in X_filtered\n",
        "                     distances = np.array([euclidean(point, other_point) for other_point in X_filtered])\n",
        "\n",
        "                     # Sort distances and get the k-th smallest distance (excluding the distance to itself)\n",
        "                     sorted_distances = np.sort(distances)\n",
        "                     # Ensure k_for_density_calculation is less than the number of points in X_filtered minus 1\n",
        "                     if k_for_density_calculation < len(X_filtered):\n",
        "                          # Find the index of the point itself in sorted_distances (it will be 0)\n",
        "                          self_distance_index = np.where(sorted_distances == 0)[0]\n",
        "                          if len(self_distance_index) > 0:\n",
        "                               # If the k-th neighbor is the point itself (distance 0), take the next one\n",
        "                               if k_for_density_calculation < len(sorted_distances):\n",
        "                                   kth_distance = sorted_distances[k_for_density_calculation] # k-th smallest distance (0-indexed), skipping the 0 distance\n",
        "                               else:\n",
        "                                    # Fallback if not enough points after excluding self\n",
        "                                    kth_distance = sorted_distances[-1] if len(sorted_distances) > 0 else 0.0 # Handle empty case\n",
        "                          else:\n",
        "                               # Should not happen for a point in the dataset, but as a fallback\n",
        "                               kth_distance = sorted_distances[k_for_density_calculation -1] # k-th smallest distance (0-indexed)\n",
        "\n",
        "                     else:\n",
        "                          # If k is larger than or equal to the number of points, use the distance to the furthest point (excluding self)\n",
        "                          distances_excluding_self = distances[distances > 0]\n",
        "                          kth_distance = np.max(distances_excluding_self) if len(distances_excluding_self) > 0 else 0.0\n",
        "\n",
        "                     kth_neighbor_distances_cluster.append(kth_distance)\n",
        "\n",
        "                # Add Kth_Neighbor_Distance to the points_in_cluster_info DataFrame\n",
        "                points_in_cluster_info['Kth_Neighbor_Distance'] = kth_neighbor_distances_cluster\n",
        "\n",
        "\n",
        "                # --- Calculate Inverse Confidence for points within this cluster ---\n",
        "                # Ensure 'Individual_Confidence_Minority' is available in points_in_cluster_info\n",
        "                if 'Individual_Confidence_Minority' not in points_in_cluster_info.columns:\n",
        "                     print(\"Error: 'Individual_Confidence_Minority' not found in points_in_cluster_info. Rerun previous steps.\")\n",
        "                     continue # Skip this cluster if confidence info is missing\n",
        "\n",
        "                # Inverse confidence in the minority class\n",
        "                # Use a small epsilon to avoid division by zero if confidence is 1.0\n",
        "                epsilon_confidence = 1e-6\n",
        "                points_in_cluster_info['Inverse_Minority_Confidence'] = 1 / (points_in_cluster_info['Individual_Confidence_Minority'] + epsilon_confidence)\n",
        "\n",
        "\n",
        "                # --- Allocate Samples to Points within the Cluster based on Combined Inverse Density and Inverse Confidence ---\n",
        "\n",
        "                distances_cluster = points_in_cluster_info['Kth_Neighbor_Distance'].values\n",
        "                inverse_confidences_cluster = points_in_cluster_info['Inverse_Minority_Confidence'].values\n",
        "\n",
        "                # Calculate allocation weights based on the combination of inverse density and inverse confidence\n",
        "                # Option 1: Simple multiplication (High density AND High inverse confidence get high weight)\n",
        "                # allocation_weights_cluster = (1 / (distances_cluster + epsilon_density_allocation)) * inverse_confidences_cluster\n",
        "\n",
        "                # Option 2: Sum of normalized inverse density and normalized inverse confidence\n",
        "                epsilon_density_allocation = 1e-6 # Small value to avoid division by zero\n",
        "                inverse_distances_cluster = 1 / (distances_cluster + epsilon_density_allocation)\n",
        "\n",
        "                # Normalize inverse distances within the cluster\n",
        "                total_inverse_distance_cluster = np.sum(inverse_distances_cluster)\n",
        "                if total_inverse_distance_cluster > 0:\n",
        "                     normalized_inverse_distances_cluster = inverse_distances_cluster / total_inverse_distance_cluster\n",
        "                else:\n",
        "                     normalized_inverse_distances_cluster = np.zeros_like(inverse_distances_cluster)\n",
        "\n",
        "                # Normalize inverse confidences within the cluster\n",
        "                total_inverse_confidence_cluster = np.sum(inverse_confidences_cluster)\n",
        "                if total_inverse_confidence_cluster > 0:\n",
        "                     normalized_inverse_confidences_cluster = inverse_confidences_cluster / total_inverse_confidence_cluster\n",
        "                else:\n",
        "                     normalized_inverse_confidences_cluster = np.zeros_like(inverse_confidences_cluster)\n",
        "\n",
        "                # Combine normalized weights (e.g., simple average)\n",
        "                allocation_weights_cluster = (normalized_inverse_distances_cluster + normalized_inverse_confidences_cluster) / 2\n",
        "\n",
        "\n",
        "                # Normalize the combined weights so they sum to 1 within the cluster\n",
        "                # This is important to ensure the total allocated samples match the cluster's requirement\n",
        "                total_combined_weight_cluster = np.sum(allocation_weights_cluster)\n",
        "                if total_combined_weight_cluster > 0:\n",
        "                    normalized_combined_weights_cluster = allocation_weights_cluster / total_combined_weight_cluster\n",
        "                else:\n",
        "                    normalized_combined_weights_cluster = np.zeros_like(allocation_weights_cluster)\n",
        "\n",
        "\n",
        "                # Calculate number of samples to generate per point in this cluster\n",
        "                num_samples_to_generate_per_point_cluster = np.round(normalized_combined_weights_cluster * num_samples_to_generate_from_cluster).astype(int)\n",
        "\n",
        "\n",
        "                # Adjust for potential rounding errors\n",
        "                total_generated_in_this_cluster = np.sum(num_samples_to_generate_per_point_cluster)\n",
        "                difference_in_cluster_allocation = num_samples_to_generate_from_cluster - total_generated_in_this_cluster\n",
        "\n",
        "                if difference_in_cluster_allocation != 0 and len(points_in_cluster_info) > 0:\n",
        "                     # Get indices of points with highest weights (in descending order)\n",
        "                     highest_weight_indices_cluster = np.argsort(normalized_combined_weights_cluster)[::-1]\n",
        "                     for i in range(abs(difference_in_cluster_allocation)):\n",
        "                          target_index_in_cluster_points_df = highest_weight_indices_cluster[i % len(highest_weight_indices_cluster)]\n",
        "                          if difference_in_cluster_allocation > 0:\n",
        "                               num_samples_to_generate_per_point_cluster[target_index_in_cluster_points_df] += 1\n",
        "                          else:\n",
        "                               if num_samples_to_generate_per_point_cluster[target_index_in_cluster_points_df] > 0:\n",
        "                                    num_samples_to_generate_per_point_cluster[target_index_in_cluster_points_df] -= 1\n",
        "\n",
        "\n",
        "                points_in_cluster_info['Samples_to_Generate_From_Point'] = num_samples_to_generate_per_point_cluster\n",
        "\n",
        "\n",
        "                # --- Generate Synthetic Samples from points in this cluster ---\n",
        "                source_point_indices_in_filtered_pool = []\n",
        "                for idx, point_info in points_in_cluster_info.iterrows():\n",
        "                     filtered_idx = int(point_info['Filtered_Index'])\n",
        "                     samples_to_generate_from_this_point = int(point_info['Samples_to_Generate_From_Point'])\n",
        "                     source_point_indices_in_filtered_pool.extend([filtered_idx] * samples_to_generate_from_this_point)\n",
        "\n",
        "\n",
        "                # Shuffle the pool of indices to randomize the order of generation from source points\n",
        "                np.random.shuffle(source_point_indices_in_filtered_pool)\n",
        "\n",
        "\n",
        "                print(f\"  Generating {len(source_point_indices_in_filtered_pool)} samples from {len(points_in_cluster_info)} source points in Cluster {cluster_label}...\")\n",
        "\n",
        "\n",
        "                for source_filtered_idx in source_point_indices_in_filtered_pool:\n",
        "                    source_point = X_filtered[source_filtered_idx]\n",
        "\n",
        "                    # Find k_for_synthesis_neighbors nearest neighbors in X_filtered\n",
        "                    distances = np.array([euclidean(source_point, other_point) for other_point in X_filtered])\n",
        "                    # Get indices of sorted distances, excluding the source point itself\n",
        "                    if k_for_synthesis_neighbors < len(X_filtered):\n",
        "                         nearest_neighbor_indices_in_filtered = np.argsort(distances)[1:k_for_synthesis_neighbors+1]\n",
        "                    else:\n",
        "                         nearest_neighbor_indices_in_filtered = np.argsort(distances)[1:]\n",
        "\n",
        "\n",
        "                    available_neighbors_indices = nearest_neighbor_indices_in_filtered\n",
        "                    if len(available_neighbors_indices) == 0:\n",
        "                         # Fallback if no neighbors found\n",
        "                         continue\n",
        "\n",
        "\n",
        "                    # Randomly select a neighbor from the k nearest neighbors\n",
        "                    neighbor_filtered_idx = np.random.choice(available_neighbors_indices)\n",
        "                    neighbor_point = X_filtered[neighbor_filtered_idx]\n",
        "                    neighbor_class_val = y_filtered[neighbor_filtered_idx] # Use the actual class label\n",
        "\n",
        "                    # Determine alpha based on your existing rules\n",
        "                    # Get info for the source point (it's a target minority point within a cluster)\n",
        "                    # Use final_filtered_info_df which has updated confidence scores\n",
        "                    source_info = final_filtered_info_df[final_filtered_info_df['Filtered_Index'] == source_filtered_idx]\n",
        "                    if source_info.empty:\n",
        "                         print(f\"Warning: Could not find info for source point index {source_filtered_idx}. Skipping.\")\n",
        "                         continue\n",
        "                    source_info = source_info.iloc[0] # Get the first (and should be only) row\n",
        "                    source_zone = source_info['Zone']\n",
        "                    source_minority_subgroup = source_info['Minority_Subgroup'] # Use the new subgroup\n",
        "                    source_individual_confidence_minority = source_info['Individual_Confidence_Minority']\n",
        "                    source_individual_confidence_majority = 1 - source_individual_confidence_minority\n",
        "\n",
        "\n",
        "                    # Get info for the neighbor point\n",
        "                    # Use final_filtered_info_df which has updated confidence scores\n",
        "                    neighbor_info = final_filtered_info_df[final_filtered_info_df['Filtered_Index'] == neighbor_filtered_idx]\n",
        "                    if neighbor_info.empty:\n",
        "                         print(f\"Warning: Could not find info for neighbor point index {neighbor_filtered_idx}. Skipping.\")\n",
        "                         continue\n",
        "                    neighbor_info = neighbor_info.iloc[0] # Get the first (and should be only) row\n",
        "                    neighbor_zone = neighbor_info['Zone']\n",
        "                    neighbor_original_class = neighbor_info['Original_Class']\n",
        "                    neighbor_individual_confidence_minority = neighbor_info['Individual_Confidence_Minority']\n",
        "                    neighbor_individual_confidence_majority = neighbor_info['Individual_Confidence_Majority']\n",
        "\n",
        "                    # Determine neighbor subgroup for minority neighbors\n",
        "                    neighbor_minority_subgroup = None\n",
        "                    if neighbor_original_class == 'Minority':\n",
        "                        if neighbor_zone == 'dangerous':\n",
        "                            neighbor_minority_subgroup = 'Dangerous Minority'\n",
        "                        elif neighbor_zone == 'safe' and neighbor_individual_confidence_minority < probability_threshold:\n",
        "                            neighbor_minority_subgroup = 'Safe Minority Low Conf'\n",
        "                        elif neighbor_zone == 'safe' and neighbor_individual_confidence_minority >= probability_threshold:\n",
        "                            neighbor_minority_subgroup = 'Safe Minority High Conf'\n",
        "                        elif neighbor_zone == 'noise': # Added Noise Minority subgroup\n",
        "                            neighbor_minority_subgroup = 'Noise Minority'\n",
        "\n",
        "\n",
        "                    # Calculate uncertainties based on the user's definition\n",
        "                    # Source is always Minority/Positive in this phase, so use minority confidence for source uncertainty\n",
        "                    uncertainty_source = 1 - source_individual_confidence_minority\n",
        "\n",
        "                    # Neighbor uncertainty depends on neighbor's class\n",
        "                    # MODIFIED: ALWAYS use Individual Confidence Minority for neighbor uncertainty\n",
        "                    uncertainty_neighbor = 1 - neighbor_individual_confidence_minority\n",
        "\n",
        "\n",
        "                    # Apply the NEW adaptive alpha rules based on user's specifications\n",
        "\n",
        "                    # Rule 1: Use formula from original Condition i)\n",
        "                    # IF Source is Dangerous Minority AND Neighbor is (Any Majority OR Noise Minority)\n",
        "                    rule1_match = False\n",
        "                    if source_minority_subgroup == 'Dangerous Minority':\n",
        "                         if neighbor_original_class == 'Majority' or neighbor_minority_subgroup == 'Noise Minority':\n",
        "                              rule1_match = True\n",
        "\n",
        "                    # OR IF Source is Safe Minority Low Conf AND Neighbor is (Safe Minority High Conf OR Noise Minority OR Any Majority)\n",
        "                    elif source_minority_subgroup == 'Safe Minority Low Conf':\n",
        "                         if neighbor_minority_subgroup == 'Safe Minority High Conf' or neighbor_minority_subgroup == 'Noise Minority' or neighbor_original_class == 'Majority':\n",
        "                              rule1_match = True\n",
        "\n",
        "\n",
        "                    # Rule 2: Use formula from original Condition ii)\n",
        "                    # IF Source is Dangerous Minority AND Neighbor is (Safe Minority Low Conf OR Safe Minority High Conf OR Dangerous Minority)\n",
        "                    rule2_match = False\n",
        "                    if source_minority_subgroup == 'Dangerous Minority':\n",
        "                         if neighbor_minority_subgroup == 'Safe Minority Low Conf' or neighbor_minority_subgroup == 'Safe Minority High Conf' or neighbor_minority_subgroup == 'Dangerous Minority':\n",
        "                              rule2_match = True\n",
        "\n",
        "                    # OR IF Source is Safe Minority Low Conf AND Neighbor is (Dangerous Minority OR Safe Minority Low Conf)\n",
        "                    elif source_minority_subgroup == 'Safe Minority Low Conf':\n",
        "                         if neighbor_minority_subgroup == 'Dangerous Minority' or neighbor_minority_subgroup == 'Safe Minority Low Conf':\n",
        "                              rule2_match = True\n",
        "\n",
        "                    # OR IF Source is Safe Minority High Conf AND Neighbor is (Any Majority OR Dangerous Minority OR Noise Minority OR Safe Minority Low Conf OR Safe Minority High Conf)\n",
        "                    elif source_minority_subgroup == 'Safe Minority High Conf':\n",
        "                          if neighbor_original_class == 'Majority' or neighbor_minority_subgroup == 'Dangerous Minority' or neighbor_minority_subgroup == 'Noise Minority' or neighbor_minority_subgroup == 'Safe Minority Low Conf' or neighbor_minority_subgroup == 'Safe Minority High Conf':\n",
        "                              rule2_match = True\n",
        "\n",
        "\n",
        "                    # Apply the formula based on which rule is met\n",
        "                    if rule1_match:\n",
        "                         alpha = uncertainty_source / (2 * (uncertainty_source + uncertainty_neighbor + epsilon))\n",
        "                         # print(f\"  Generated sample with alpha from Rule 1 ({alpha:.4f}) for Source: {source_minority_subgroup}, Neighbor: {neighbor_original_class}/{neighbor_minority_subgroup}\") # Optional: for debugging\n",
        "\n",
        "                    elif rule2_match:\n",
        "                         alpha = uncertainty_source / (uncertainty_source + uncertainty_neighbor + epsilon)\n",
        "                         # print(f\"  Generated sample with alpha from Rule 2 ({alpha:.4f}) for Source: {source_minority_subgroup}, Neighbor: {neighbor_original_class}/{neighbor_minority_subgroup}\") # Optional: for debugging\n",
        "\n",
        "                    else:\n",
        "                         # Fallback or handle cases not covered by the rules\n",
        "                         # Based on user request, we should not use random alpha.\n",
        "                         # However, if a source-neighbor combination doesn't match any rule,\n",
        "                         # we need a defined behavior. A simple approach is to use alpha = 0.5,\n",
        "                         # or potentially use one of the existing formulas as a default.\n",
        "                         # Let's use alpha = 0.5 as a neutral fallback for now if no rule matches.\n",
        "                         alpha = 0.5\n",
        "                         print(f\"  Warning: No specific rule matched for Source: {source_minority_subgroup}, Neighbor: {neighbor_original_class}/{neighbor_minority_subgroup}. Using alpha = {alpha:.4f}\")\n",
        "\n",
        "\n",
        "                    # Ensure alpha is between 0 and 1\n",
        "                    alpha = np.clip(alpha, 0, 1)\n",
        "\n",
        "\n",
        "                    # Generate synthetic sample using linear interpolation (Equation 22 description)\n",
        "                    synthetic_point = source_point + alpha * (neighbor_point - source_point)\n",
        "\n",
        "                    # Add the synthetic sample and label (always minority)\n",
        "                    synthetic_samples.append(synthetic_point)\n",
        "                    synthetic_labels.append(1)\n",
        "\n",
        "    print(f\"Finished generating {len(synthetic_samples)} synthetic samples.\")\n",
        "\n",
        "    # Convert the list of synthetic samples and labels to numpy arrays\n",
        "    # ONLY perform vstack if synthetic_samples is not empty\n",
        "    if synthetic_samples:\n",
        "        X_synthetic_custom = np.array(synthetic_samples)\n",
        "        y_synthetic_custom = np.array(synthetic_labels)\n",
        "\n",
        "        # Combine the original filtered data with the generated synthetic data\n",
        "        X_balanced_custom = np.vstack((X_filtered, X_synthetic_custom))\n",
        "        y_balanced_custom = np.hstack((y_filtered, y_synthetic_custom))\n",
        "\n",
        "        print(f\"\\nCombined oversampled data shape: {X_balanced_custom.shape}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo synthetic samples generated. X_balanced_custom and y_balanced_custom are the same as X_filtered and y_filtered.\")\n",
        "        X_balanced_custom = X_filtered\n",
        "        y_balanced_custom = y_filtered\n",
        "\n",
        "\n",
        "else: # synthetic_samples_needed == 0\n",
        "    # --- No Balancing Needed ---\n",
        "    print(f\"Filtered data is already balanced or has equal class counts ({len(y_filtered[y_filtered == 0])} Majority, {len(y_filtered[y_filtered == 1])}) Minority). No balancing applied.\")\n",
        "    X_balanced_custom = X_filtered\n",
        "    y_balanced_custom = y_filtered\n",
        "    print(f\"Balanced data shape (same as filtered): {X_balanced_custom.shape}\")\n",
        "\n",
        "\n",
        "# Display the class distribution of the balanced data\n",
        "unique_balanced, counts_balanced = np.unique(y_balanced_custom, return_counts=True)\n",
        "balanced_class_distribution = dict(zip(unique_balanced, counts_balanced))\n",
        "print(f\"\\nBalanced data class distribution: {balanced_class_distribution}\")\n",
        "\n",
        "# Update the variables used later to refer to the balanced dataset\n",
        "X_oversampled_custom = X_balanced_custom\n",
        "y_oversampled_custom = y_balanced_custom"
      ],
      "metadata": {
        "id": "k95r6KNHJUMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94dbc80e"
      },
      "source": [
        "### Evaluating Classifiers on Original, Filtered, and Balanced Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score, roc_auc_score, f1_score, matthews_corrcoef, balanced_accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold # Import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier # Import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier # Import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "import pandas as pd\n",
        "\n",
        "# Define classifiers to use - MODIFIED TO INCLUDE ONLY RBF SVM, MLP, and Decision Tree\n",
        "classifiers = {\n",
        "    # \"Random Forest\": RandomForestClassifier(random_state=42), # Removed\n",
        "    # \"Linear SVM\": LinearSVC(random_state=42), # Removed\n",
        "    \"RBF SVM\": SVC(gamma='auto', random_state=42, probability=True), # probability=True needed for roc_auc_score and G-mean\n",
        "    # \"Naive Bayes\": GaussianNB(), # Removed\n",
        "    \"MLP\": MLPClassifier(random_state=42, max_iter=1000),\n",
        "    # \"KNN\": KNeighborsClassifier(), # Removed\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42) # Add DecisionTreeClassifier\n",
        "}\n",
        "\n",
        "# Define the metrics to report - MODIFIED TO INCLUDE ONLY THE REQUESTED METRICS\n",
        "metrics_to_report = [\"Recall\", \"ROC AUC\", \"G-mean\", \"F1-score\", \"Balanced Accuracy\"]\n",
        "\n",
        "# Define the number of splits for Stratified K-Fold Cross-Validation\n",
        "n_splits = 5 # You can adjust the number of folds as needed\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Function to evaluate a classifier using Stratified K-Fold\n",
        "def evaluate_classifier_cv(clf, X_train_data, y_train_data, X_test_data, y_test_data, metrics_list, dataset_name):\n",
        "    \"\"\"\n",
        "    Evaluates a classifier using Stratified K-Fold Cross-Validation on the training data\n",
        "    and then evaluates the model trained on the full training data on the test data.\n",
        "\n",
        "    Args:\n",
        "        clf: The classifier to evaluate.\n",
        "        X_train_data (np.ndarray): The training data features.\n",
        "        y_train_data (np.ndarray): The training data labels.\n",
        "        X_test_data (np.ndarray): The test data features.\n",
        "        y_test_data (np.ndarray): The test data labels.\n",
        "        metrics_list (list): A list of metric names to calculate.\n",
        "        dataset_name (str): Name of the dataset (e.g., 'Original', 'Filtered').\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - cv_results (dict): Dictionary of average CV scores for each metric.\n",
        "            - test_results (dict): Dictionary of scores on the separate test set.\n",
        "    \"\"\"\n",
        "    cv_scores = {metric: [] for metric in metrics_list}\n",
        "    test_results = {}\n",
        "\n",
        "    print(f\"\\n--- {clf.__class__.__name__} (Trained on {dataset_name}) ---\")\n",
        "    print(f\"Performing {n_splits}-Fold Stratified Cross-Validation on {dataset_name} training data...\")\n",
        "\n",
        "    # Perform Stratified K-Fold Cross-Validation on the training data\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(X_train_data, y_train_data)):\n",
        "        X_train_fold, X_val_fold = X_train_data[train_index], X_train_data[val_index]\n",
        "        y_train_fold, y_val_fold = y_train_data[train_index], y_train_data[val_index]\n",
        "\n",
        "        # Create a fresh instance of the classifier for each fold to avoid state leakage\n",
        "        fold_clf = type(clf)(**clf.get_params()) # Create new instance with same parameters\n",
        "\n",
        "        try:\n",
        "            fold_clf.fit(X_train_fold, y_train_fold)\n",
        "            y_pred_val = fold_clf.predict(X_val_fold)\n",
        "\n",
        "            # Calculate metrics for the current fold\n",
        "            fold_metrics = {}\n",
        "            if \"Recall\" in metrics_list:\n",
        "                fold_metrics[\"Recall\"] = recall_score(y_val_fold, y_pred_val)\n",
        "            if \"ROC AUC\" in metrics_list:\n",
        "                try:\n",
        "                    if hasattr(fold_clf, \"predict_proba\"):\n",
        "                        y_prob_val = fold_clf.predict_proba(X_val_fold)[:, 1]\n",
        "                        fold_metrics[\"ROC AUC\"] = roc_auc_score(y_val_fold, y_prob_val)\n",
        "                    elif hasattr(fold_clf, \"decision_function\"):\n",
        "                         y_prob_val = fold_clf.decision_function(X_val_fold)\n",
        "                         fold_metrics[\"ROC AUC\"] = roc_auc_score(y_val_fold, y_prob_val)\n",
        "                    else:\n",
        "                        fold_metrics[\"ROC AUC\"] = np.nan\n",
        "                except Exception as metric_e:\n",
        "                     print(f\"    Fold {fold+1}: Error calculating ROC AUC: {metric_e}\")\n",
        "                     fold_metrics[\"ROC AUC\"] = np.nan\n",
        "            if \"G-mean\" in metrics_list:\n",
        "                 try:\n",
        "                      fold_metrics[\"G-mean\"] = geometric_mean_score(y_val_fold, y_pred_val)\n",
        "                 except Exception as gmean_e:\n",
        "                      print(f\"    Fold {fold+1}: Error calculating G-mean: {gmean_e}\")\n",
        "                      fold_metrics[\"G-mean\"] = np.nan\n",
        "            if \"F1-score\" in metrics_list:\n",
        "                 fold_metrics[\"F1-score\"] = f1_score(y_val_fold, y_pred_val)\n",
        "            if \"Balanced Accuracy\" in metrics_list:\n",
        "                 fold_metrics[\"Balanced Accuracy\"] = balanced_accuracy_score(y_val_fold, y_pred_val)\n",
        "\n",
        "            # Append fold metrics to the list\n",
        "            for metric, score in fold_metrics.items():\n",
        "                cv_scores[metric].append(score)\n",
        "\n",
        "            print(f\"    Fold {fold+1} completed.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error during training or evaluation of Fold {fold+1}: {e}\")\n",
        "            # Append None or np.nan for metrics if an error occurred\n",
        "            for metric in metrics_list:\n",
        "                 cv_scores[metric].append(np.nan)\n",
        "\n",
        "\n",
        "    # Calculate average CV scores\n",
        "    avg_cv_results = {metric: np.nanmean(scores) for metric, scores in cv_scores.items()} # Use nanmean to handle potential errors\n",
        "    print(f\"\\nAverage CV Scores ({dataset_name} Training Data):\")\n",
        "    for metric, score in avg_cv_results.items():\n",
        "        print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "\n",
        "    # Train the classifier on the *full* training data for evaluation on the test set\n",
        "    print(f\"\\nTraining on full {dataset_name} training data for test set evaluation...\")\n",
        "    test_clf = type(clf)(**clf.get_params()) # Create a new instance for training on full data\n",
        "    try:\n",
        "        test_clf.fit(X_train_data, y_train_data)\n",
        "        y_pred_test = test_clf.predict(X_test_data)\n",
        "\n",
        "        # Calculate metrics on the separate Test Data\n",
        "        if \"Recall\" in metrics_list:\n",
        "            test_results[\"Recall\"] = recall_score(y_test_data, y_pred_test)\n",
        "        if \"ROC AUC\" in metrics_list:\n",
        "            try:\n",
        "                if hasattr(test_clf, \"predict_proba\"):\n",
        "                    y_prob_test = test_clf.predict_proba(X_test_data)[:, 1]\n",
        "                    test_results[\"ROC AUC\"] = roc_auc_score(y_test_data, y_prob_test)\n",
        "                elif hasattr(test_clf, \"decision_function\"):\n",
        "                     y_prob_test = test_clf.decision_function(X_test_data)\n",
        "                     test_results[\"ROC AUC\"] = roc_auc_score(y_test_data, y_prob_test)\n",
        "                else:\n",
        "                    test_results[\"ROC AUC\"] = np.nan\n",
        "            except Exception as metric_e:\n",
        "                 print(f\"  Error calculating ROC AUC on test set: {metric_e}\")\n",
        "                 test_results[\"ROC AUC\"] = np.nan\n",
        "\n",
        "        if \"G-mean\" in metrics_list:\n",
        "             try:\n",
        "                  test_results[\"G-mean\"] = geometric_mean_score(y_test_data, y_pred_test)\n",
        "             except Exception as gmean_e:\n",
        "                  print(f\"  Error calculating G-mean on test set: {gmean_e}\")\n",
        "                  test_results[\"G-mean\"] = np.nan\n",
        "        if \"F1-score\" in metrics_list:\n",
        "            test_results[\"F1-score\"] = f1_score(y_test_data, y_pred_test)\n",
        "        if \"Balanced Accuracy\" in metrics_list:\n",
        "            test_results[\"Balanced Accuracy\"] = balanced_accuracy_score(y_test_data, y_pred_test)\n",
        "\n",
        "        print(f\"\\nTest Set Evaluation Results (Trained on full {dataset_name} Training Data):\")\n",
        "        for metric, score in test_results.items():\n",
        "            print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error training on full {dataset_name} data or evaluating on test set: {e}\")\n",
        "        test_results = {metric: np.nan for metric in metrics_list} # Set all test metrics to NaN on error\n",
        "\n",
        "\n",
        "    return avg_cv_results, test_results\n",
        "\n",
        "\n",
        "# --- Evaluate classifiers trained on Original Data ---\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Evaluating Classifiers Trained on ORIGINAL Data\")\n",
        "results_original_cv = {}\n",
        "results_original_test = {}\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    cv_res, test_res = evaluate_classifier_cv(clf, X_train, y_train, X_test, y_test, metrics_to_report, 'Original')\n",
        "    results_original_cv[name] = cv_res\n",
        "    results_original_test[name] = test_res\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Evaluate classifiers trained on Filtered Data ---\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Evaluating Classifiers Trained on FILTERED Data\")\n",
        "results_filtered_cv = {}\n",
        "results_filtered_test = {}\n",
        "\n",
        "# Define X_train_filtered and y_train_filtered to use X_filtered and y_filtered\n",
        "X_train_filtered = X_filtered\n",
        "y_train_filtered = y_filtered\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    cv_res, test_res = evaluate_classifier_cv(clf, X_train_filtered, y_train_filtered, X_test, y_test, metrics_to_report, 'Filtered')\n",
        "    results_filtered_cv[name] = cv_res\n",
        "    results_filtered_test[name] = test_res\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Evaluate classifiers trained on Custom Oversampled Data ---\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Evaluating Classifiers Trained on Custom Oversampled Data\")\n",
        "results_custom_oversampled_cv = {} # CV results on the oversampled data itself\n",
        "results_custom_oversampled_test = {} # Test results after training on oversampled data\n",
        "\n",
        "# Ensure X_oversampled_custom and y_oversampled_custom are available from previous steps.\n",
        "# Ensure X_test and y_test are available from the original train/test split.\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    # For the custom oversampled data, the CV is performed on the oversampled training set.\n",
        "    # The final test evaluation is on the original, unseen test set.\n",
        "    cv_res, test_res = evaluate_classifier_cv(clf, X_oversampled_custom, y_oversampled_custom, X_test, y_test, metrics_to_report, 'Custom Oversampled')\n",
        "    results_custom_oversampled_cv[name] = cv_res\n",
        "    results_custom_oversampled_test[name] = test_res\n",
        "\n",
        "\n",
        "# Create DataFrames for CV results\n",
        "original_cv_results_df = pd.DataFrame(results_original_cv).T\n",
        "filtered_cv_results_df = pd.DataFrame(results_filtered_cv).T\n",
        "custom_oversampled_cv_results_df = pd.DataFrame(results_custom_oversampled_cv).T # CV results on custom oversampled data\n",
        "\n",
        "# Create DataFrames for Test results (trained on full data)\n",
        "original_test_results_df = pd.DataFrame(results_original_test).T\n",
        "filtered_test_results_df = pd.DataFrame(results_filtered_test).T\n",
        "custom_oversampled_test_results_df = pd.DataFrame(results_custom_oversampled_test).T\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(\"\\nAverage Cross-Validation Results (Trained and Evaluated on Training Data Folds):\")\n",
        "print(\"\\nOriginal Training Data CV Results:\")\n",
        "display(original_cv_results_df)\n",
        "\n",
        "print(\"\\nFiltered Training Data CV Results:\")\n",
        "display(filtered_cv_results_df)\n",
        "\n",
        "print(\"\\nCustom Oversampled Training Data CV Results:\")\n",
        "display(custom_oversampled_cv_results_df)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(\"\\nTest Results (Trained on Full Training Data, Evaluated on Separate Test Set):\")\n",
        "print(\"\\nOriginal Training Data Test Results:\")\n",
        "display(original_test_results_df)\n",
        "\n",
        "print(\"\\nFiltered Training Data Test Results:\")\n",
        "display(filtered_test_results_df)\n",
        "\n",
        "print(\"\\nCustom Oversampled Training Data Test Results:\")\n",
        "display(custom_oversampled_test_results_df)\n",
        "\n",
        "\n",
        "# --- Save DataFrames to Excel ---\n",
        "\n",
        "# Define the path for the Excel file\n",
        "excel_file_path = 'evaluation_results_cv.xlsx' # New filename to differentiate\n",
        "\n",
        "# Create an ExcelWriter object\n",
        "# Use mode='w' to create or overwrite the file\n",
        "with pd.ExcelWriter(excel_file_path, mode='w') as writer:\n",
        "    # Save each DataFrame to a different sheet\n",
        "    if not original_cv_results_df.empty:\n",
        "        original_cv_results_df.to_excel(writer, sheet_name='Original_CV_Results')\n",
        "    if not original_test_results_df.empty:\n",
        "        original_test_results_df.to_excel(writer, sheet_name='Original_Test_Results')\n",
        "    if not filtered_cv_results_df.empty:\n",
        "        filtered_cv_results_df.to_excel(writer, sheet_name='Filtered_CV_Results')\n",
        "    if not filtered_test_results_df.empty:\n",
        "        filtered_test_results_df.to_excel(writer, sheet_name='Filtered_Test_Results')\n",
        "    if not custom_oversampled_cv_results_df.empty:\n",
        "        custom_oversampled_cv_results_df.to_excel(writer, sheet_name='Custom_Oversampled_CV_Results')\n",
        "    if not custom_oversampled_test_results_df.empty:\n",
        "        custom_oversampled_test_results_df.to_excel(writer, sheet_name='Custom_Oversampled_Test_Results')\n",
        "    # Add the train_points_info_updated DataFrame to a new sheet (if it exists)\n",
        "    if 'train_points_info_updated' in locals() and not train_points_info_updated.empty:\n",
        "        train_points_info_updated.to_excel(writer, sheet_name='Train_Points_Info_Updated')\n",
        "    # Add the final_filtered_info_df DataFrame to a new sheet (if it exists)\n",
        "    if 'final_filtered_info_df' in locals() and not final_filtered_info_df.empty:\n",
        "        final_filtered_info_df.to_excel(writer, sheet_name='Final_Filtered_Info')\n",
        "\n",
        "\n",
        "print(f\"\\nEvaluation results (including CV) saved to '{excel_file_path}'\")"
      ],
      "metadata": {
        "id": "_-0vfWe9Ju_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBFAS FINISH"
      ],
      "metadata": {
        "id": "BBd66jVgMIZ8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}